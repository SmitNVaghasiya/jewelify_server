{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["umZR6n4LKJjJ","O65NAzvN0hZg","MtQ6rCQc0_rH"],"mount_file_id":"11oWMUI6Cvc0_nqPYrivzRUJCRciPCz-g","authorship_tag":"ABX9TyM2c0B5VLT21nzRipFgS3lB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1️⃣ Extract Features for Separate Objects (Face, Necklace, Earrings)**\n","\n","---\n","\n"],"metadata":{"id":"vCappx8vKQyL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Wfo-13dKhUj","executionInfo":{"status":"ok","timestamp":1740569286433,"user_tz":-330,"elapsed":17786,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"5e906ecb-13a9-4b23-ff55-816eb4152c88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import os\n","# from PIL import Image\n","\n","# def jpg_convertor(input_folder, output_folder):\n","#     # Allowed image extensions\n","#     allowed_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.bmp']\n","\n","#     # Get all files in the input folder\n","#     files = os.listdir(input_folder)\n","\n","#     # Filter out files that are not images\n","#     image_files = [f for f in files if os.path.splitext(f)[1].lower() in allowed_extensions]\n","\n","#     # Iterate over all images and save them as JPG format\n","#     for image_file in image_files:\n","#         # Build the full path to the image file\n","#         image_path = os.path.join(input_folder, image_file)\n","\n","#         try:\n","#             # Open the image using PIL (Pillow)\n","#             with Image.open(image_path) as img:\n","#                 # Create the new filename with .jpg extension (converted to JPG)\n","#                 new_filename = f\"{os.path.splitext(image_file)[0]}.jpg\"\n","#                 new_file_path = os.path.join(output_folder, new_filename)\n","\n","#                 # Convert the image to RGB (required for saving as JPG)\n","#                 img = img.convert('RGB')\n","\n","#                 # Save the image in JPG format, but with a .jpg extension\n","#                 img.save(new_file_path, 'JPEG')\n","\n","#         except Exception as e:\n","#             # Print the original image path if an error occurs\n","#             print(f\"Failed to process image: {image_path}. Error: {str(e)}\")\n","\n","\n","# # List of input and output folders\n","# folders = [\n","#     \"/content/drive/MyDrive/Jewelify/src/sorted/Necklace with earings_sorted\",\n","#     \"/content/drive/MyDrive/Jewelify/src/sorted/combined_sorted\",\n","#     \"/content/drive/MyDrive/Jewelify/src/sorted/earrings_sorted\",\n","#     \"/content/drive/MyDrive/Jewelify/src/sorted/face_sorted\",\n","#     \"/content/drive/MyDrive/Jewelify/src/sorted/necklace_sorted\"\n","# ]\n","\n","# # Iterate through each input folder and create the corresponding output folder\n","# for input_folder in folders:\n","#     output_folder = f\"/content/drive/MyDrive/Jewelify/src/Jpg/{os.path.basename(input_folder)}_jpg\"\n","\n","#     # Create the output folder if it doesn't exist\n","#     if not os.path.exists(output_folder):\n","#         os.makedirs(output_folder)\n","\n","#     # Call the function to save images\n","#     jpg_convertor(input_folder, output_folder)\n","#     print(f\"Saved images from {input_folder} to {output_folder}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7DTpqs3tPCU","executionInfo":{"status":"ok","timestamp":1739379033283,"user_tz":-330,"elapsed":22700,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"65cef09a-a918-4426-b318-10548ba55e4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved images from /content/drive/MyDrive/Jewelify/src/sorted/Necklace with earings_sorted to /content/drive/MyDrive/Jewelify/src/Jpg/Necklace with earings_sorted_jpg\n","Saved images from /content/drive/MyDrive/Jewelify/src/sorted/combined_sorted to /content/drive/MyDrive/Jewelify/src/Jpg/combined_sorted_jpg\n","Saved images from /content/drive/MyDrive/Jewelify/src/sorted/earrings_sorted to /content/drive/MyDrive/Jewelify/src/Jpg/earrings_sorted_jpg\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Saved images from /content/drive/MyDrive/Jewelify/src/sorted/face_sorted to /content/drive/MyDrive/Jewelify/src/Jpg/face_sorted_jpg\n","Saved images from /content/drive/MyDrive/Jewelify/src/sorted/necklace_sorted to /content/drive/MyDrive/Jewelify/src/Jpg/necklace_sorted_jpg\n"]}]},{"cell_type":"code","source":["import os\n","import csv\n","from natsort import natsorted  # Import natural sorting\n","\n","# Define the folder containing images\n","folder_path = \"/content/drive/MyDrive/Jewelify/src/Jpg/combined_sorted_jpg\"  # Change this to your actual path\n","\n","# Supported image formats\n","IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"}\n","\n","# Define CSV file path\n","csv_file = \"combined_suitability.csv\"\n","\n","# Collect image names\n","image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in IMAGE_EXTENSIONS]\n","\n","# Sort using natural order (full_1, full_2, ..., full_10, full_11, etc.)\n","image_files = natsorted(image_files)\n","\n","# Write to CSV file\n","with open(csv_file, mode=\"w\", newline=\"\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Image Name\", \"Suitability Rating\"])  # Header row\n","\n","    for img in image_files:\n","        writer.writerow([img, \"\"])  # Empty rating (to be filled manually)\n","\n","print(f\"CSV file '{csv_file}' created successfully with natural sorting.\")"],"metadata":{"id":"mfZxIkxpZv1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2️⃣ Train Multi-Object Compatibility Model (Using All-Objects Images & CSV File)**"],"metadata":{"id":"_RyTwT-QKM_l"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO0-tDPcJM1s","executionInfo":{"status":"ok","timestamp":1740569268675,"user_tz":-330,"elapsed":214142,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"fd526d1c-e2fa-4c2c-d8a3-5fc0a14b0ff0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","✅ Saved all features successfully!\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Model\n","\n","# Define image size and directories\n","IMG_SIZE = (224, 224)\n","necklace_dir = \"/content/drive/MyDrive/Jewelify/src/Jpg/necklace_sorted_jpg\"\n","earring_dir = \"/content/drive/MyDrive/Jewelify/src/Jpg/earrings_sorted_jpg\"\n","face_dir = \"/content/drive/MyDrive/Jewelify/src/Jpg/face_sorted_jpg\"\n","full_image_dir_1 = \"/content/drive/MyDrive/Jewelify/src/Jpg/Necklace with earings_sorted_jpg\"\n","full_image_dir_2 = \"/content/drive/MyDrive/Jewelify/src/Jpg/combined_sorted_jpg\"\n","\n","# Load MobileNetV2 for feature extraction\n","base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n","feature_extractor = Model(inputs=base_model.input, outputs=tf.keras.layers.GlobalAveragePooling2D()(base_model.output))\n","\n","# Function to preprocess image and normalize\n","def preprocess_image(img_path):\n","    img = load_img(img_path, target_size=IMG_SIZE)\n","    img = img_to_array(img) / 255.0  # Normalize\n","    return img\n","\n","# Function to extract features from an image directory\n","def extract_features(image_dir):\n","    image_files = sorted(os.listdir(image_dir))\n","    image_features = {}\n","\n","    for img_name in image_files:\n","        img_path = os.path.join(image_dir, img_name)\n","        if not os.path.isfile(img_path):\n","            continue  # Skip if it's not a valid file\n","\n","        img_array = preprocess_image(img_path)\n","        img_array_expanded = np.expand_dims(img_array, axis=0)\n","        features = feature_extractor.predict(img_array_expanded, verbose=0)[0]  # Extract features\n","\n","        image_features[img_name] = features  # Store features using image name as key\n","\n","    return image_features\n","\n","# Extract features from the different directories\n","necklace_features = extract_features(necklace_dir)\n","earring_features = extract_features(earring_dir)\n","face_features = extract_features(face_dir)\n","multi_object_features_1 = extract_features(full_image_dir_1)\n","multi_object_features_2 = extract_features(full_image_dir_2)\n","\n","# ✅ Save extracted features correctly\n","np.save(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/necklace_features.npy\", necklace_features, allow_pickle=True)\n","np.save(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/earring_features.npy\", earring_features, allow_pickle=True)\n","np.save(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/face_features.npy\", face_features, allow_pickle=True)\n","np.save(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/pairwise_features.npy\", multi_object_features_1, allow_pickle=True)\n","np.save(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/multi_object_features.npy\", multi_object_features_2, allow_pickle=True)\n","\n","print(\"✅ Saved all features successfully!\")"]},{"cell_type":"markdown","source":["**Preprocessing dataset**\n","\n","Averaging all the ratings i have got"],"metadata":{"id":"dKQRm4KGgCK0"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Function to process the compatibility scores, calculate average, and categorize (normalize 0-2 to 0-1)\n","def process_csv(csv_file, processed_csv_file):\n","    # Load the CSV file\n","    df = pd.read_csv(csv_file)\n","\n","    # Ensure column names are correct\n","    image_names = df[\"Image Name\"].values\n","    suitability_ratings = df.iloc[:, 1:].values  # All columns after \"Image Name\"\n","\n","    # Convert ratings to numeric (in case of empty cells or incorrect inputs)\n","    df.iloc[:, 1:] = df.iloc[:, 1:].apply(pd.to_numeric, errors=\"coerce\")\n","\n","    # Normalize ratings from 0-2 to 0-1\n","    df.iloc[:, 1:] = df.iloc[:, 1:] / 2.0\n","\n","    # Calculate the average rating for each image (now between 0 and 1)\n","    df[\"Average Rating\"] = df.iloc[:, 1:].mean(axis=1)\n","\n","    # Calculate standard deviation (to measure consensus level)\n","    df[\"Consensus (Std Dev)\"] = df.iloc[:, 1:].std(axis=1)\n","\n","    # Categorize the images based on average rating (0 to 1 range)\n","    def categorize(avg_rating):\n","        if avg_rating >= 0.875:\n","            return \"Highly Compatible\"\n","        elif 0.75 <= avg_rating < 0.875:\n","            return \"Very Compatible\"\n","        elif 0.625 <= avg_rating < 0.75:\n","            return \"Compatible\"\n","        elif 0.5 <= avg_rating < 0.625:\n","            return \"Slightly Compatible\"\n","        elif 0.25 <= avg_rating < 0.5:\n","            return \"Neutral\"\n","        else:\n","            return \"Not Compatible\"\n","\n","    df[\"Category\"] = df[\"Average Rating\"].apply(categorize)\n","\n","    # Save the processed dataset\n","    df.to_csv(processed_csv_file, index=False)\n","\n","    print(f\"Processed CSV saved as '{processed_csv_file}'.\")\n","\n","# File paths\n","multi_object_csv_file = \"/content/drive/MyDrive/Jewelify/Trained Features/csv/combined_suitability.csv\"\n","processed_multi_object_csv = \"/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv\"\n","\n","# Process the CSV file\n","process_csv(multi_object_csv_file, processed_multi_object_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAau2e5nBM-h","executionInfo":{"status":"ok","timestamp":1740569370045,"user_tz":-330,"elapsed":73,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"9ed61824-c44d-43a7-a248-6c444c751a6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed CSV saved as '/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv'.\n"]}]},{"cell_type":"code","source":["# import pandas as pd\n","\n","# # Function to process the compatibility scores, calculate average, and categorize\n","# def process_csv(csv_file, processed_csv_file):\n","#     # Load the CSV file\n","#     df = pd.read_csv(csv_file)\n","\n","#     # Ensure column names are correct\n","#     # Assuming the first column is \"Image Name\" and the subsequent columns are suitability ratings\n","#     image_names = df[\"Image Name\"].values\n","#     suitability_ratings = df.iloc[:, 1:].values  # All columns after \"Image Name\" (suitability ratings)\n","\n","#     # Convert ratings to numeric (in case of empty cells or incorrect inputs)\n","#     df.iloc[:, 1:] = df.iloc[:, 1:].apply(pd.to_numeric, errors=\"coerce\")\n","\n","#     # Calculate the average rating for each image\n","#     df[\"Average Rating\"] = df.iloc[:, 1:].mean(axis=1)\n","\n","#     # Calculate standard deviation (to measure consensus level)\n","#     df[\"Consensus (Std Dev)\"] = df.iloc[:, 1:].std(axis=1)\n","\n","#     # Categorize the images based on average rating into the 6 categories you defined\n","#     def categorize(avg_rating):\n","#         if avg_rating >= 1.75:\n","#             return \"Highly Compatible\"\n","#         elif 1.5 <= avg_rating < 1.75:\n","#             return \"Very Compatible\"\n","#         elif 1.25 <= avg_rating < 1.5:\n","#             return \"Compatible\"\n","#         elif 1.0 <= avg_rating < 1.25:\n","#             return \"Slightly Compatible\"\n","#         elif 0.5 <= avg_rating < 1.0:\n","#             return \"Neutral\"\n","#         else:\n","#             return \"Not Compatible\"\n","\n","#     df[\"Category\"] = df[\"Average Rating\"].apply(categorize)\n","\n","#     # Save the processed dataset\n","#     df.to_csv(processed_csv_file, index=False)\n","\n","#     print(f\"Processed CSV saved as '{processed_csv_file}'.\")\n","\n","# # File paths\n","# multi_object_csv_file = \"/content/drive/MyDrive/Jewelify/Trained Features/csv/combined_suitability.csv\"\n","# processed_multi_object_csv = \"/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv\"\n","\n","# # Process the CSV file\n","# process_csv(multi_object_csv_file, processed_multi_object_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hoFhE-ed6ua","executionInfo":{"status":"ok","timestamp":1740569327433,"user_tz":-330,"elapsed":54,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"fc51a185-c0a3-411f-9497-2336f1eb201b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed CSV saved as '/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv'.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv\")\n","\n","print(df.shape)\n","print(df.columns)\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-9G5ZrVu9V-","executionInfo":{"status":"ok","timestamp":1740569373673,"user_tz":-330,"elapsed":29,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"9c0e09b2-c7b3-4c42-b6e7-8e6a5eadca8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(279, 6)\n","Index(['Image Name', 'Suitability Rating 1', 'Suitability Rating 2',\n","       'Average Rating', 'Consensus (Std Dev)', 'Category'],\n","      dtype='object')\n","           Image Name  Suitability Rating 1  Suitability Rating 2  \\\n","0      combined_1.jpg                  1.00                  1.00   \n","1      combined_2.jpg                  0.50                  1.00   \n","2      combined_3.jpg                  0.50                  0.50   \n","3      combined_4.jpg                  0.75                  0.75   \n","4      combined_5.jpg                  1.00                  1.00   \n","..                ...                   ...                   ...   \n","274  combined_275.jpg                  0.00                  0.00   \n","275  combined_276.jpg                  1.00                  1.00   \n","276  combined_277.jpg                  1.00                  1.00   \n","277  combined_278.jpg                  1.00                  1.00   \n","278  combined_279.jpg                  1.00                  1.00   \n","\n","     Average Rating  Consensus (Std Dev)             Category  \n","0              1.00                 0.00    Highly Compatible  \n","1              0.75                 0.25      Very Compatible  \n","2              0.50                 0.00  Slightly Compatible  \n","3              0.75                 0.00      Very Compatible  \n","4              1.00                 0.00    Highly Compatible  \n","..              ...                  ...                  ...  \n","274            0.00                 0.00       Not Compatible  \n","275            1.00                 0.00    Highly Compatible  \n","276            1.00                 0.00    Highly Compatible  \n","277            1.00                 0.00    Highly Compatible  \n","278            1.00                 0.00    Highly Compatible  \n","\n","[279 rows x 6 columns]\n"]}]},{"cell_type":"markdown","source":["All object Features old code"],"metadata":{"id":"fcdklVWVdpP4"}},{"cell_type":"code","source":[],"metadata":{"id":"TE4BNWojNjNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Load extracted features from 200 images (full-object images)\n","multi_object_features = np.load(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/multi_object_features.npy\", allow_pickle=True).item()\n","\n","# Load compatibility scores CSV for 200 images\n","df = pd.read_csv(\"/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv\")\n","image_names = df[\"Image Name\"].values\n","labels = df[\"Average Rating\"].values\n","\n","# Prepare feature dataset\n","X = np.array([multi_object_features[img] for img in image_names])\n","\n","# Normalize features (scaling to [0, 1] range)\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Rescale the labels to [0, 1] (assuming your labels are between 0 and 2)\n","label_scaler = MinMaxScaler(feature_range=(0, 1))\n","y_scaled = label_scaler.fit_transform(labels.reshape(-1, 1))\n","\n","# Define model\n","input_layer = Input(shape=(X_scaled.shape[1],))\n","dense = Dense(256, activation='relu')(input_layer)\n","dense = Dense(128, activation='relu')(dense)\n","output_layer = Dense(1, activation='linear')(dense)  # Linear activation for regression (continuous values)\n","\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","\n","# Train model\n","model.fit(X_scaled, y_scaled, epochs=20, batch_size=32, validation_split=0.2)\n","\n","# OR save the model in .keras format (single file)\n","model.save(\"/content/drive/MyDrive/Jewelify/Trained Features/keras/multi_object_compatibility_model.keras\")\n","print(\"Model trained and saved in modern format.\")"],"metadata":{"id":"46vhxXmlJzip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training both pairwise and all objects using same function"],"metadata":{"id":"fHme9BlCduS1"}},{"cell_type":"code","source":[],"metadata":{"id":"mzXTltIGrEOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import pandas as pd\n","# import tensorflow as tf\n","# from tensorflow.keras.layers import Input, Dense\n","# from tensorflow.keras.models import Model\n","\n","# # Load extracted features for necklace-earring combined images\n","# pairwise_features = np.load(\"pairwise_features.npy\", allow_pickle=True).item()  # Combined image features\n","\n","# # Load compatibility CSV (pairs manually labeled as compatible or not)\n","# df = pd.read_csv(\"pairwise_compatibility.csv\")  # Contains {image_name, compatibility_score}\n","# image_names = df[\"image_name\"].values\n","# labels = df[\"compatibility_score\"].values  # 0 or 1\n","\n","# # Prepare dataset\n","# X = np.array([pairwise_features[img] for img in image_names])  # Load feature vectors\n","# y = np.array(labels)  # Labels\n","\n","# # Define model\n","# input_layer = Input(shape=(X.shape[1],))\n","# dense = Dense(256, activation='relu')(input_layer)\n","# dense = Dense(128, activation='relu')(dense)\n","# output_layer = Dense(1, activation='sigmoid')(dense)\n","\n","# model = Model(inputs=input_layer, outputs=output_layer)\n","# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# # Train model\n","# model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n","\n","# # Save trained model\n","# model.save(\"pairwise_compatibility_model.keras\", save_format=\"keras\")\n","# print(\"Pairwise compatibility model trained and saved.\")"],"metadata":{"id":"HVTl0OiFdNM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PXJHKYQ0q5Ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Er-dzDxodNJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **3️⃣ Predict Compatibility for All 150×150 Pairs Using Step 2 Model**"],"metadata":{"id":"umZR6n4LKJjJ"}},{"cell_type":"markdown","source":["this code using pairwise npy file and saving it"],"metadata":{"id":"bToysjBtbm8n"}},{"cell_type":"code","source":[],"metadata":{"id":"xr8C-Si6bmNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import pandas as pd\n","# import tensorflow as tf\n","# from tensorflow.keras.layers import Input, Dense, Concatenate\n","# from tensorflow.keras.models import Model\n","\n","# # Load extracted features\n","# necklace_features = np.load(\"necklace_features.npy\", allow_pickle=True).item()\n","# earring_features = np.load(\"earring_features.npy\", allow_pickle=True).item()\n","\n","# # Load compatibility CSV (pairs manually labeled as compatible or not)\n","# df = pd.read_csv(\"pairwise_compatibility.csv\")  # Contains {necklace_image, earring_image, compatibility_score}\n","# necklaces = df[\"necklace_image\"].values\n","# earrings = df[\"earring_image\"].values\n","# labels = df[\"compatibility_score\"].values  # 0 or 1\n","\n","# # Prepare dataset\n","# X_necklaces = np.array([necklace_features[n] for n in necklaces])\n","# X_earrings = np.array([earring_features[e] for e in earrings])\n","# X = np.concatenate([X_necklaces, X_earrings], axis=1)  # Merge features\n","# y = np.array(labels)\n","\n","# # Define model\n","# input_layer = Input(shape=(X.shape[1],))\n","# dense = Dense(256, activation='relu')(input_layer)\n","# dense = Dense(128, activation='relu')(dense)\n","# output_layer = Dense(1, activation='sigmoid')(dense)\n","\n","# model = Model(inputs=input_layer, outputs=output_layer)\n","# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# # Train model\n","# model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n","\n","# # Save trained model\n","# model.save(\"pairwise_compatibility_model.keras\", save_format=\"keras\")\n","# print(\"Pairwise compatibility model trained and saved.\")"],"metadata":{"id":"D3X_8LI-JzfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code is only predicting using the pair wise saved features"],"metadata":{"id":"3Au5N1USKFcW"}},{"cell_type":"code","source":["# import itertools\n","# import numpy as np\n","# import pandas as pd\n","# from tensorflow.keras.models import load_model\n","\n","# # Load trained model\n","# model = load_model(\"pairwise_compatibility_model.keras\")\n","\n","# # Load extracted features\n","# necklace_features = np.load(\"necklace_features.npy\", allow_pickle=True).item()\n","# earring_features = np.load(\"earring_features.npy\", allow_pickle=True).item()\n","# face_features = np.load(\"face_features.npy\", allow_pickle=True).item()\n","\n","# # Generate all possible combinations (150×150×150)\n","# pair_combinations = list(itertools.product(face_features.keys(), necklace_features.keys(), earring_features.keys()))\n","\n","# # Prepare for batch processing\n","# batch_size = 512  # Adjust based on GPU memory\n","# results = []\n","\n","# for i in range(0, len(pair_combinations), batch_size):\n","#     batch_pairs = pair_combinations[i:i+batch_size]\n","\n","#     # Extract feature vectors and concatenate them\n","#     batch_faces = np.array([face_features[f] for f, n, e in batch_pairs])\n","#     batch_necklaces = np.array([necklace_features[n] for f, n, e in batch_pairs])\n","#     batch_earrings = np.array([earring_features[e] for f, n, e in batch_pairs])\n","\n","#     # Concatenate features\n","#     batch_features = np.concatenate([batch_faces, batch_necklaces, batch_earrings], axis=1)\n","\n","#     # Predict compatibility\n","#     predictions = model.predict(batch_features, batch_size=32)\n","\n","#     # Store results\n","#     for (face, necklace, earring), score in zip(batch_pairs, predictions.flatten()):\n","#         results.append((face, necklace, earring, score))\n","\n","# # Save compatibility results\n","# df_results = pd.DataFrame(results, columns=[\"face_image\", \"necklace_image\", \"earring_image\", \"compatibility_score\"])\n","# df_results.to_csv(\"face_jewelry_compatibility_results.csv\", index=False)\n","\n","# print(\"Predictions saved to face_jewelry_compatibility_results.csv\")"],"metadata":{"id":"OE2s5DfhJzcl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code is using all the features pair wise and the combined features"],"metadata":{"id":"eRikd1fgKB9q"}},{"cell_type":"code","source":["import itertools\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import load_model\n","\n","# Load trained models\n","pairwise_model = load_model(\"pairwise_compatibility_model.keras\")  # Necklace-Earring model\n","multi_object_model = load_model(\"multi_object_compatibility_model.keras\")  # Face-Necklace-Earring model\n","\n","# Load extracted features\n","necklace_features = np.load(\"necklace_features.npy\", allow_pickle=True).item()\n","earring_features = np.load(\"earring_features.npy\", allow_pickle=True).item()\n","face_features = np.load(\"face_features.npy\", allow_pickle=True).item()\n","\n","# Generate all possible combinations (150×150×150)\n","pair_combinations = list(itertools.product(face_features.keys(), necklace_features.keys(), earring_features.keys()))\n","\n","# Prepare for batch processing\n","batch_size = 512  # Adjust based on GPU memory\n","results = []\n","\n","for i in range(0, len(pair_combinations), batch_size):\n","    batch_pairs = pair_combinations[i:i+batch_size]\n","\n","    # Extract feature vectors\n","    batch_faces = np.array([face_features[f] for f, n, e in batch_pairs])\n","    batch_necklaces = np.array([necklace_features[n] for f, n, e in batch_pairs])\n","    batch_earrings = np.array([earring_features[e] for f, n, e in batch_pairs])\n","\n","    # Predict necklace-earring compatibility\n","    pairwise_features = np.concatenate([batch_necklaces, batch_earrings], axis=1)\n","    pairwise_predictions = pairwise_model.predict(pairwise_features, batch_size=32)\n","\n","    # Predict full face-jewelry compatibility\n","    full_features = np.concatenate([batch_faces, batch_necklaces, batch_earrings], axis=1)\n","    multi_object_predictions = multi_object_model.predict(full_features, batch_size=32)\n","\n","    # Final compatibility score (average both models)\n","    final_scores = (pairwise_predictions.flatten() + multi_object_predictions.flatten()) / 2\n","\n","    # Store results\n","    for (face, necklace, earring), score in zip(batch_pairs, final_scores):\n","        results.append((face, necklace, earring, score))\n","\n","# Save compatibility results\n","df_results = pd.DataFrame(results, columns=[\"face_image\", \"necklace_image\", \"earring_image\", \"compatibility_score\"])\n","df_results.to_csv(\"face_jewelry_compatibility_results.csv\", index=False)\n","\n","print(\"Predictions saved to face_jewelry_compatibility_results.csv\")\n"],"metadata":{"id":"6GujmU68JzaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.models import load_model\n","\n","# Load trained model\n","model = load_model(\"pairwise_compatibility_model.keras\")\n","\n","# Load extracted feature files\n","necklace_features = np.load(\"necklace_features.npy\", allow_pickle=True).item()\n","earring_features = np.load(\"earring_features.npy\", allow_pickle=True).item()\n","face_features = np.load(\"face_features.npy\", allow_pickle=True).item()\n","\n","def predict_compatibility(face_image, necklace_image, earring_image):\n","    \"\"\"Predict compatibility score for a given face, necklace, and earring image.\"\"\"\n","\n","    # Ensure the images exist in feature data\n","    if face_image not in face_features:\n","        print(f\"Error: Face image '{face_image}' not found in extracted features.\")\n","        return None\n","    if necklace_image not in necklace_features:\n","        print(f\"Error: Necklace image '{necklace_image}' not found in extracted features.\")\n","        return None\n","    if earring_image not in earring_features:\n","        print(f\"Error: Earring image '{earring_image}' not found in extracted features.\")\n","        return None\n","\n","    # Extract feature vectors\n","    face_vector = face_features[face_image]\n","    necklace_vector = necklace_features[necklace_image]\n","    earring_vector = earring_features[earring_image]\n","\n","    # Concatenate features\n","    input_vector = np.concatenate([face_vector, necklace_vector, earring_vector]).reshape(1, -1)\n","\n","    # Predict compatibility score\n","    score = model.predict(input_vector)[0][0]\n","\n","    return score\n","\n","# Example usage:\n","face_img = \"face1.jpg\"\n","necklace_img = \"necklace1.jpg\"\n","earring_img = \"earring1.jpg\"\n","\n","compatibility_score = predict_compatibility(face_img, necklace_img, earring_img)\n","if compatibility_score is not None:\n","    print(f\"Compatibility Score for ({face_img}, {necklace_img}, {earring_img}): {compatibility_score:.4f}\")"],"metadata":{"id":"bkpUROw-JzX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wZOJhvfLJzVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0WzEjMDUJzTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rDagImDSJ9bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fv7g7lwCJ85T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Predicting compability of Necklace and Earrings"],"metadata":{"id":"O65NAzvN0hZg"}},{"cell_type":"code","source":["# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Dense, Dropout\n","# from tensorflow.keras.optimizers import Adam\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import accuracy_score\n","\n","# # ========================\n","# # **Define Paths**\n","# # ========================\n","# pairwise_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/pairwise_features.npy\"\n","# earring_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/earring_features.npy\"\n","# necklace_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/necklace_features.npy\"\n","# model_save_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/jewelry_pairwise_model.keras\"\n","\n","# # ========================\n","# # **Load Extracted Features**\n","# # ========================\n","# pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()  # Load as dictionary\n","# earring_features = np.load(earring_features_path, allow_pickle=True).item()  # Load as dictionary\n","# necklace_features = np.load(necklace_features_path, allow_pickle=True).item()  # Load as dictionary\n","\n","# # Ensure features are stored as NumPy arrays\n","# earring_features = {k: np.array(v) for k, v in earring_features.items()}\n","# necklace_features = {k: np.array(v) for k, v in necklace_features.items()}\n","\n","# # ========================\n","# # **Prepare Training Data**\n","# # ========================\n","# X, y = [], []\n","\n","# # Dictionary to check if a pair exists in ground truth (for labeling)\n","# existing_pairs = set(tuple(v) for v in pairwise_features.values())\n","\n","# # Compare every necklace feature with every earring feature\n","# for necklace_name, necklace_vec in necklace_features.items():\n","#     for earring_name, earring_vec in earring_features.items():\n","#         combined_features = np.concatenate((necklace_vec, earring_vec))  # Merge both features\n","#         pair_key = (necklace_name, earring_name)  # Key for checking\n","\n","#         # Check if this pair exists in ground truth\n","#         label = 1 if pair_key in existing_pairs else 0\n","\n","#         X.append(combined_features)\n","#         y.append(label)\n","\n","# # Convert to NumPy arrays\n","# X = np.array(X)\n","# y = np.array(y)\n","\n","# # ========================\n","# # **Train-Test Split**\n","# # ========================\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# # ========================\n","# # **Build Neural Network Model**\n","# # ========================\n","# model = Sequential([\n","#     Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],)),\n","#     Dropout(0.2),\n","#     Dense(256, activation=\"relu\"),\n","#     Dropout(0.2),\n","#     Dense(128, activation=\"relu\"),\n","#     Dense(1, activation=\"sigmoid\")  # Binary classification (compatible or not)\n","# ])\n","\n","# # Compile model\n","# model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# # ========================\n","# # **Train Model**\n","# # ========================\n","# epochs = 20\n","# batch_size = 16\n","\n","# history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","#                     epochs=epochs, batch_size=batch_size, verbose=1)\n","\n","# # ========================\n","# # **Evaluate Model**\n","# # ========================\n","# y_pred = (model.predict(X_test) > 0.5).astype(int)\n","# accuracy = accuracy_score(y_test, y_pred)\n","\n","# print(\"\\n🔍 **Model Performance** 🔍\")\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# # ========================\n","# # **Save Model**\n","# # ========================\n","# model.save(model_save_path)\n","# print(f\"\\n✅ Model saved as {model_save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVsKE4JH0s9b","executionInfo":{"status":"ok","timestamp":1739369391908,"user_tz":-330,"elapsed":144700,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"b04c4711-80fa-43d6-c070-09651b33263a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - accuracy: 0.9940 - loss: 0.0144 - val_accuracy: 1.0000 - val_loss: 2.1566e-08\n","Epoch 2/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.9895e-07 - val_accuracy: 1.0000 - val_loss: 1.8085e-09\n","Epoch 3/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.5363e-07 - val_accuracy: 1.0000 - val_loss: 3.0385e-10\n","Epoch 4/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.6741e-07 - val_accuracy: 1.0000 - val_loss: 7.5487e-11\n","Epoch 5/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1063e-07 - val_accuracy: 1.0000 - val_loss: 2.0902e-11\n","Epoch 6/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4343e-07 - val_accuracy: 1.0000 - val_loss: 4.7453e-12\n","Epoch 7/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.3607e-08 - val_accuracy: 1.0000 - val_loss: 1.9274e-12\n","Epoch 8/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.5028e-08 - val_accuracy: 1.0000 - val_loss: 6.4429e-13\n","Epoch 9/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.5631e-09 - val_accuracy: 1.0000 - val_loss: 2.7827e-13\n","Epoch 10/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.0140e-09 - val_accuracy: 1.0000 - val_loss: 1.4498e-13\n","Epoch 11/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.1043e-09 - val_accuracy: 1.0000 - val_loss: 3.9566e-14\n","Epoch 12/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.5613e-09 - val_accuracy: 1.0000 - val_loss: 2.6121e-14\n","Epoch 13/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.8904e-10 - val_accuracy: 1.0000 - val_loss: 1.8597e-14\n","Epoch 14/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.6802e-10 - val_accuracy: 1.0000 - val_loss: 1.2365e-14\n","Epoch 15/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.7497e-10 - val_accuracy: 1.0000 - val_loss: 8.9248e-15\n","Epoch 16/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.5050e-10 - val_accuracy: 1.0000 - val_loss: 5.4494e-15\n","Epoch 17/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.3664e-10 - val_accuracy: 1.0000 - val_loss: 4.0867e-15\n","Epoch 18/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0069e-10 - val_accuracy: 1.0000 - val_loss: 3.4335e-15\n","Epoch 19/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1407e-10 - val_accuracy: 1.0000 - val_loss: 2.8214e-15\n","Epoch 20/20\n","\u001b[1m1133/1133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.7817e-10 - val_accuracy: 1.0000 - val_loss: 1.7719e-15\n","\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\n","🔍 **Model Performance** 🔍\n","Accuracy: 1.0000\n","\n","✅ Model saved as /content/drive/MyDrive/Jewelify/Trained Features/keras/jewelry_pairwise_model.keras\n"]}]},{"cell_type":"markdown","source":["I have to make this pair based model such that it can compare two different images one of Necklace and other of the earrings and predict if they are suitable for each other or not"],"metadata":{"id":"fuX6smnZgM7a"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Model\n","\n","# ========================\n","# **Load Trained Model**\n","# ========================\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/jewelry_pairwise_model.keras\"\n","regressor = load_model(model_path)\n","\n","# ========================\n","# **Feature Extraction Function**\n","# ========================\n","def extract_features(img_path):\n","    \"\"\"Extract features using MobileNetV2 with GlobalAveragePooling2D.\"\"\"\n","    try:\n","        img_size = (224, 224)\n","\n","        # Load MobileNetV2 with GlobalAveragePooling2D (same as training)\n","        base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","        feature_extractor = Model(inputs=base_model.input, outputs=tf.keras.layers.GlobalAveragePooling2D()(base_model.output))\n","\n","        img = image.load_img(img_path, target_size=img_size)\n","        img_array = image.img_to_array(img)\n","        img_array = np.expand_dims(img_array, axis=0)\n","        img_array = preprocess_input(img_array)\n","\n","        features = feature_extractor.predict(img_array, verbose=0)  # Extract correct-sized features\n","        return features.reshape(1, -1)  # Ensure correct input shape\n","    except Exception as e:\n","        print(f\"⚠️ Error extracting features: {e}\")\n","        return None\n","\n","# ========================\n","# **Categorize Suitability Score**\n","# ========================\n","def categorize_suitability(score):\n","    \"\"\"Convert numerical suitability score into categories.\"\"\"\n","    if score < 0.5:\n","        return \"❌ Very Bad\"\n","    elif 0.5 <= score < 1.0:\n","        return \"⚠️ Bad\"\n","    elif 1.0 <= score < 1.5:\n","        return \"😐 Neutral\"\n","    elif 1.5 <= score < 2.0:\n","        return \"✅ Good\"\n","    else:\n","        return \"🌟 Very Good\"\n","\n","# ========================\n","# **Predict Jewelry Compatibility**\n","# ========================\n","def predict_compatibility(necklace_img_path, earring_img_path):\n","    \"\"\"Predict compatibility score for a given necklace-earring pair.\"\"\"\n","\n","    if not os.path.exists(necklace_img_path) or not os.path.exists(earring_img_path):\n","        print(\"❌ Error: One or both image files not found.\")\n","        return None\n","\n","    # Extract features for both images\n","    necklace_features = extract_features(necklace_img_path)\n","    earring_features = extract_features(earring_img_path)\n","\n","    if necklace_features is None or earring_features is None:\n","        print(\"❌ Error: Could not extract features.\")\n","        return None\n","\n","    # Concatenate both feature vectors (expected input shape: (1, 2560))\n","    combined_features = np.concatenate((necklace_features, earring_features), axis=1)\n","\n","    # Force execution on GPU if available\n","    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n","        prediction = regressor.predict(combined_features, verbose=0)[0][0]\n","\n","    category = categorize_suitability(prediction)\n","\n","    print(f\"\\n🔮 **Predicted Suitability Score**: {prediction:.2f} ({category})\")\n","    return prediction, category\n","\n","# ========================\n","# **Example Usage**\n","# ========================\n","if __name__ == \"__main__\":\n","    necklace_path = \"/content/nkimz22823_c_2.jpg\"  # Change this to your necklace image\n","    earring_path = \"/content/download.jpg\"  # Change this to your earring image\n","    predict_compatibility(necklace_path, earring_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zggw_y4G0_Ca","executionInfo":{"status":"ok","timestamp":1739369929762,"user_tz":-330,"elapsed":10973,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"70e99a57-3d22-410c-d78d-3e036893aafd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔮 **Predicted Suitability Score**: 0.00 (❌ Very Bad)\n"]}]},{"cell_type":"markdown","source":["# Predicting the Multi model objects"],"metadata":{"id":"MtQ6rCQc0_rH"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"/content/drive/MyDrive/Jewelify/Trained Features/csv/processed_multi_object_jewelry_ratings.csv\")\n","df.head()"],"metadata":{"id":"bqL_aPallxt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# missing_entries = 0\n","# for index, row in df.iterrows():\n","#     image_name = row[\"Image Name\"]\n","\n","#     face_vec = face_features.get(image_name, None)\n","#     necklace_vec = necklace_features.get(image_name, None)\n","#     earring_vec = earring_features.get(image_name, None)\n","#     multi_vec = multi_object_features.get(image_name, None)\n","#     pairwise_vec = pairwise_features.get(image_name, None)\n","\n","#     if None in [face_vec, necklace_vec, earring_vec, multi_vec, pairwise_vec]:\n","#         missing_entries += 1\n","\n","# print(f\"❌ Missing feature entries: {missing_entries} / {len(df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ke3QAzBXC4M","executionInfo":{"status":"ok","timestamp":1739383914082,"user_tz":-330,"elapsed":724,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"9749ae4b-f100-4df8-e1c1-386abba191dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["❌ Missing feature entries: 279 / 279\n"]}]},{"cell_type":"code","source":["# print(df.columns)"],"metadata":{"id":"gwQdbBKxXNJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["face_features = np.load(\"/content/drive/MyDrive/Jewelify/Trained Features/pandas/face_features.npy\", allow_pickle=True)"],"metadata":{"id":"aNkX62UgXVHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if len(X) == 0:\n","#     print(\"⚠️ No valid data points were added. Check feature extraction.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ock6lGaeXn1D","executionInfo":{"status":"ok","timestamp":1739384066180,"user_tz":-330,"elapsed":7,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"160dd768-cdf2-42be-ec0c-35056b4a8091"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ No valid data points were added. Check feature extraction.\n"]}]},{"cell_type":"code","source":["# import numpy as np\n","# import pandas as pd\n","# base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","\n","# # Feature files (all from pandas folder)\n","# face_features_path = f\"{base_path}/pandas/face_features.npy\"\n","# necklace_features_path = f\"{base_path}/pandas/necklace_features.npy\"\n","# earring_features_path = f\"{base_path}/pandas/earring_features.npy\"\n","# multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","# pairwise_features_path = f\"{base_path}/pandas/pairwise_features.npy\"\n","\n","# # Compatibility scores\n","# csv_ratings_path = f\"{base_path}/csv/processed_multi_object_jewelry_ratings.csv\"\n","\n","# # Load all extracted features\n","# face_features = np.load(face_features_path, allow_pickle=True).item()\n","# necklace_features = np.load(necklace_features_path, allow_pickle=True).item()\n","# earring_features = np.load(earring_features_path, allow_pickle=True).item()\n","# multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","# pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","\n","# # Ensure features are NumPy arrays\n","# face_features = {k: np.array(v) for k, v in face_features.items()}\n","# necklace_features = {k: np.array(v) for k, v in necklace_features.items()}\n","# earring_features = {k: np.array(v) for k, v in earring_features.items()}\n","# multi_object_features = {k: np.array(v) for k, v in multi_object_features.items()}\n","# pairwise_features = {k: np.array(v) for k, v in pairwise_features.items()}\n","\n","# print(\"Face feature keys:\", list(face_features.keys())[:5])\n","# print(\"Necklace feature keys:\", list(necklace_features.keys())[:5])\n","# print(\"Earring feature keys:\", list(earring_features.keys())[:5])\n","# print(\"Multi-object feature keys:\", list(multi_object_features.keys())[:5])\n","# print(\"Pairwise feature keys:\", list(pairwise_features.keys())[:5])\n","\n","# # Print counts\n","# print(f\"Face features: {len(face_features)}\")\n","# print(f\"Necklace features: {len(necklace_features)}\")\n","# print(f\"Earring features: {len(earring_features)}\")\n","# print(f\"Multi-object features: {len(multi_object_features)}\")\n","# print(f\"Pairwise features: {len(pairwise_features)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjcCOE0UTW4J","executionInfo":{"status":"ok","timestamp":1740491607797,"user_tz":-330,"elapsed":2075,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"a8d11808-07b1-42dd-a9a5-4a5631840758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Face feature keys: ['face_1.jpg', 'face_10.jpg', 'face_100.jpg', 'face_101.jpg', 'face_102.jpg']\n","Necklace feature keys: ['necklace_1.jpg', 'necklace_10.jpg', 'necklace_100.jpg', 'necklace_101.jpg', 'necklace_102.jpg']\n","Earring feature keys: ['earrings_1.jpg', 'earrings_10.jpg', 'earrings_100.jpg', 'earrings_101.jpg', 'earrings_102.jpg']\n","Multi-object feature keys: ['combined_1.jpg', 'combined_10.jpg', 'combined_100.jpg', 'combined_101.jpg', 'combined_102.jpg']\n","Pairwise feature keys: ['Necklace with earings_1.jpg', 'Necklace with earings_10.jpg', 'Necklace with earings_100.jpg', 'Necklace with earings_101.jpg', 'Necklace with earings_102.jpg']\n","Face features: 147\n","Necklace features: 150\n","Earring features: 150\n","Multi-object features: 279\n","Pairwise features: 227\n"]}]},{"cell_type":"markdown","source":["**✅ Best Fixes**\n","🔹 Option 1: Use Only Available Features (Easier)\n","Modify the script so it doesn’t require all five features. Instead, replace missing ones with zeros."],"metadata":{"id":"cvIKVGXtYffp"}},{"cell_type":"code","source":["# X, y = [], []\n","\n","# for index, row in df.iterrows():\n","#     image_name = row[\"Image Name\"]\n","#     compatibility_score = row[\"Average Rating\"]\n","\n","#     # Retrieve available features (allow missing ones)\n","#     face_vec = face_features.get(image_name)\n","#     necklace_vec = necklace_features.get(image_name)\n","#     earring_vec = earring_features.get(image_name)\n","#     multi_vec = multi_object_features.get(image_name)\n","#     pairwise_vec = pairwise_features.get(image_name)\n","\n","#     # Define default size (adjust based on actual feature size)\n","#     default_size = 128  # Change this if feature vectors are different sizes\n","\n","#     def safe_feature(feature, size=default_size):\n","#         return feature if feature is not None else np.zeros(size)\n","\n","#     # Concatenate available features (fill missing ones with zeros)\n","#     combined_features = np.concatenate([\n","#         safe_feature(face_vec),\n","#         safe_feature(necklace_vec),\n","#         safe_feature(earring_vec),\n","#         safe_feature(multi_vec),\n","#         safe_feature(pairwise_vec),\n","#     ])\n","\n","#     X.append(combined_features)\n","#     y.append(compatibility_score)\n","\n","# # Convert to NumPy arrays\n","# X = np.array(X)\n","# y = np.array(y)\n","\n","# if len(X) == 0:\n","#     print(\"⚠️ No valid data points were added. Check feature extraction.\")\n","# else:\n","#     print(f\"✅ Training data prepared! Total samples: {len(X)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yufRKYBSYbxb","executionInfo":{"status":"ok","timestamp":1739384297540,"user_tz":-330,"elapsed":3,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"9503f1f3-1f1a-46f0-b9c4-b896972baf2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Training data prepared! Total samples: 279\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","\n","# ========================\n","# **Define Paths**\n","# ========================\n","base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","\n","# Feature files (all from pandas folder)\n","face_features_path = f\"{base_path}/pandas/face_features.npy\"\n","necklace_features_path = f\"{base_path}/pandas/necklace_features.npy\"\n","earring_features_path = f\"{base_path}/pandas/earring_features.npy\"\n","multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","pairwise_features_path = f\"{base_path}/pandas/pairwise_features.npy\"\n","\n","# Compatibility scores\n","csv_ratings_path = f\"{base_path}/csv/processed_multi_object_jewelry_ratings.csv\"\n","\n","# Model save path\n","model_save_path = f\"{base_path}/keras/multi_model_feature_model.keras\"\n","\n","# ========================\n","# **Load Extracted Features**\n","# ========================\n","df = pd.read_csv(csv_ratings_path)\n","\n","# Load all extracted features safely\n","face_features = np.load(face_features_path, allow_pickle=True).item()\n","necklace_features = np.load(necklace_features_path, allow_pickle=True).item()\n","earring_features = np.load(earring_features_path, allow_pickle=True).item()\n","multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","\n","# Convert features to NumPy arrays\n","face_features = {k: np.array(v) for k, v in face_features.items()}\n","necklace_features = {k: np.array(v) for k, v in necklace_features.items()}\n","earring_features = {k: np.array(v) for k, v in earring_features.items()}\n","multi_object_features = {k: np.array(v) for k, v in multi_object_features.items()}\n","pairwise_features = {k: np.array(v) for k, v in pairwise_features.items()}\n","\n","# ========================\n","# **Prepare Training Data**\n","# ========================\n","X, y = [], []\n","\n","# Define expected feature vector sizes (modify if needed)\n","default_size = 128  # Adjust this if your feature vectors have a different size\n","\n","def safe_feature(feature, size=default_size):\n","    \"\"\"Returns the feature if available; otherwise, a zero vector of the same size.\"\"\"\n","    return feature if feature is not None else np.zeros(size)\n","\n","for index, row in df.iterrows():\n","    image_name = row[\"Image Name\"]\n","    compatibility_score = row[\"Average Rating\"]\n","\n","    # Retrieve available features (replace missing ones with zeros)\n","    face_vec = safe_feature(face_features.get(image_name))\n","    necklace_vec = safe_feature(necklace_features.get(image_name))\n","    earring_vec = safe_feature(earring_features.get(image_name))\n","    multi_vec = safe_feature(multi_object_features.get(image_name))\n","    pairwise_vec = safe_feature(pairwise_features.get(image_name))\n","\n","    # Concatenate all features into a single vector\n","    combined_features = np.concatenate([face_vec, necklace_vec, earring_vec, multi_vec, pairwise_vec])\n","\n","    X.append(combined_features)\n","    y.append(compatibility_score)\n","\n","# Convert to NumPy arrays\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Check if training data is properly prepared\n","if len(X) == 0:\n","    raise ValueError(\"❌ No valid data points were added. Check feature extraction.\")\n","else:\n","    print(f\"✅ Training data prepared! Total samples: {len(X)}\")\n","\n","# ========================\n","# **Train-Test Split**\n","# ========================\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ========================\n","# **Build Neural Network Model**\n","# ========================\n","model = Sequential([\n","    Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],)),\n","    Dropout(0.2),\n","    Dense(256, activation=\"relu\"),\n","    Dropout(0.2),\n","    Dense(128, activation=\"relu\"),\n","    Dense(1, activation=\"linear\")  # Regression model (suitability score)\n","])\n","\n","# Compile model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mean_squared_error\", metrics=[\"mae\"])\n","\n","# ========================\n","# **Train Model**\n","# ========================\n","epochs = 20\n","batch_size = 16\n","\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                    epochs=epochs, batch_size=batch_size, verbose=1)\n","\n","# ========================\n","# **Evaluate Model**\n","# ========================\n","y_pred = model.predict(X_test)\n","mae = mean_absolute_error(y_test, y_pred)\n","\n","print(\"\\n🔍 **Model Performance** 🔍\")\n","print(f\"Mean Absolute Error: {mae:.4f}\")\n","\n","# ========================\n","# **Save Model**\n","# ========================\n","model.save(model_save_path)\n","print(f\"\\n✅ Model saved as {model_save_path}\")\n"],"metadata":{"id":"4n7xTxNl1Fee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model, Model\n","\n","class JewelryCompatibilityPredictor:\n","    def __init__(self, model_path):\n","        \"\"\"Initialize the model and feature extractor.\"\"\"\n","        self.model = load_model(model_path)\n","        self.img_size = (224, 224)\n","        self.device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","\n","        # Initialize feature extractor\n","        base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","        reduction_layer = tf.keras.layers.Dense(896, activation=\"relu\")\n","        global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n","        self.feature_extractor = Model(inputs=base_model.input, outputs=reduction_layer(global_avg_layer(base_model.output)))\n","\n","    def extract_features(self, img_path):\n","        \"\"\"Extract features using MobileNetV2 with GlobalAveragePooling2D and a Dense layer for size reduction.\"\"\"\n","        if not os.path.exists(img_path):\n","            print(f\"❌ Error: Image file not found - {img_path}\")\n","            return None\n","\n","        try:\n","            img = image.load_img(img_path, target_size=self.img_size)\n","            img_array = image.img_to_array(img)\n","            img_array = np.expand_dims(img_array, axis=0)\n","            img_array = preprocess_input(img_array)\n","\n","            features = self.feature_extractor.predict(img_array, verbose=0)\n","            return features.reshape(1, -1)  # Ensure correct shape (1, feature_dim)\n","        except Exception as e:\n","            print(f\"⚠️ Error extracting features from {img_path}: {e}\")\n","            return None\n","\n","    def categorize_suitability(self, score):\n","        \"\"\"Convert numerical suitability score into categories.\"\"\"\n","        if score < 0.5:\n","            return \"❌ Very Bad\"\n","        elif 0.5 <= score < 1.0:\n","            return \"⚠️ Bad\"\n","        elif 1.0 <= score < 1.5:\n","            return \"😐 Neutral\"\n","        elif 1.5 <= score < 2.0:\n","            return \"✅ Good\"\n","        else:\n","            return \"🌟 Very Good\"\n","\n","    def predict_compatibility(self, face_img_path, jewelry_img_path):\n","        \"\"\"Predict compatibility score for a given face and jewelry image.\"\"\"\n","        face_features = self.extract_features(face_img_path)\n","        jewelry_features = self.extract_features(jewelry_img_path)\n","\n","        if face_features is None or jewelry_features is None:\n","            print(\"❌ Error: Could not extract features for one or both images.\")\n","            return None\n","\n","        combined_features = np.concatenate((face_features, jewelry_features), axis=1)\n","        expected_input_shape = self.model.input_shape[1]\n","\n","        if combined_features.shape[1] != expected_input_shape:\n","            print(f\"❌ Shape Mismatch! Model expects {expected_input_shape}, but got {combined_features.shape[1]}\")\n","            return None\n","\n","        with tf.device(self.device):\n","            prediction = self.model.predict(combined_features, verbose=0)[0][0]\n","\n","        category = self.categorize_suitability(prediction)\n","        print(f\"\\n🔮 **Predicted Suitability Score**: {prediction:.2f} ({category})\")\n","        return prediction, category\n","\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/multi_model_feature_model.keras\"\n","predictor = JewelryCompatibilityPredictor(model_path)"],"metadata":{"id":"MrjkhUN3jvrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/multi_model_feature_model.keras\"\n","predictor = JewelryCompatibilityPredictor(model_path)"],"metadata":{"id":"5uSn6HXXeDgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["face_img = \"/content/WhatsApp Image 2025-02-15 at 12.29.19_36f13d96.jpg\"\n","jewelry_img = \"/content/download (4).jpg\"\n","predictor.predict_compatibility(face_img, jewelry_img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UB8WRE3grfl","executionInfo":{"status":"ok","timestamp":1739602928961,"user_tz":-330,"elapsed":1361,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"eadd9ba9-33e4-4da8-c191-bb18a0f1cd31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔮 **Predicted Suitability Score**: 1.50 (✅ Good)\n"]},{"output_type":"execute_result","data":{"text/plain":["(1.5040137, '✅ Good')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"_NEA7JM6ZJiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ozHLp0hzZJWo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **FeedForward Neural Network**\n","**Correct Code**\n","\n","This code is using the FeedForward Neural Network To train and Predict.\n","\n","This code is training the model with only **multi model** data"],"metadata":{"id":"j0A664htZJ-b"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","# ========================\n","# **Define Paths**\n","# ========================\n","base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","\n","# Feature file (only multi-object features)\n","multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","\n","# Compatibility scores (only for multi-object features)\n","csv_ratings_path = f\"{base_path}/csv/processed_multi_object_jewelry_ratings.csv\"\n","\n","# Model and scaler save paths\n","model_save_path = f\"{base_path}/keras/FNN_multi_model_feature_model.keras\"\n","scaler_save_path = f\"{base_path}/FNN_scaler.pkl\"\n","\n","# ========================\n","# **Load Extracted Features**\n","# ========================\n","try:\n","    df = pd.read_csv(csv_ratings_path)\n","    multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","except FileNotFoundError as e:\n","    raise FileNotFoundError(f\"❌ Error loading file: {e}\")\n","\n","# Convert features to NumPy arrays\n","multi_object_features = {k: np.array(v) for k, v in multi_object_features.items()}\n","\n","# ========================\n","# **Prepare Training Data**\n","# ========================\n","X, y = [], []\n","default_size = 128  # Feature vector dimensionality (assuming 128 from earlier)\n","\n","def safe_feature(feature, size=default_size):\n","    \"\"\"Returns the feature if available; otherwise, a zero vector of the same size.\"\"\"\n","    if feature is not None and len(feature) == size:\n","        return feature\n","    return np.zeros(size)\n","\n","for index, row in df.iterrows():\n","    image_name = row[\"Image Name\"]\n","    compatibility_score = row[\"Average Rating\"]\n","\n","    # Retrieve multi-object feature (replace missing with zeros)\n","    multi_vec = safe_feature(multi_object_features.get(image_name))\n","\n","    X.append(multi_vec)\n","    y.append(compatibility_score)\n","\n","# Convert to NumPy arrays\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Check if training data is properly prepared\n","if len(X) == 0:\n","    raise ValueError(\"❌ No valid data points were added. Check feature extraction or CSV data.\")\n","else:\n","    print(f\"✅ Training data prepared! Total samples: {len(X)}, Feature dimension: {X.shape[1]}\")\n","\n","# Normalize features and save the scaler\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Save the scaler to a file\n","with open(scaler_save_path, 'wb') as f:\n","    pickle.dump(scaler, f)\n","print(f\"✅ Scaler saved as {scaler_save_path}\")\n","\n","# ========================\n","# **Train-Test Split**\n","# ========================\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ========================\n","# **Build Neural Network Model**\n","# ========================\n","model = Sequential([\n","    Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n","    Dropout(0.3),\n","    Dense(128, activation=\"relu\"),\n","    Dropout(0.3),\n","    Dense(64, activation=\"relu\"),\n","    Dense(1, activation=\"linear\")  # Regression output\n","])\n","\n","# Compile model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mean_squared_error\", metrics=[\"mae\", \"mse\"])\n","\n","# ========================\n","# **Train Model**\n","# ========================\n","early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","epochs = 50  # Increased to allow early stopping to work effectively\n","batch_size = 16\n","\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                    epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n","\n","# ========================\n","# **Evaluate Model with Multiple Metrics**\n","# ========================\n","y_pred = model.predict(X_test).flatten()  # Flatten to match y_test shape\n","\n","# Calculate error metrics\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)  # Root Mean Squared Error\n","r2 = r2_score(y_test, y_pred)  # R-squared\n","\n","print(\"\\n🔍 **Model Performance** 🔍\")\n","print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","print(f\"R-squared (R²): {r2:.4f}\")\n","\n","# ========================\n","# **Save Model**\n","# ========================\n","model.save(model_save_path)\n","print(f\"\\n✅ Model saved as {model_save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgsGxAoGZJTS","executionInfo":{"status":"ok","timestamp":1740500720842,"user_tz":-330,"elapsed":17724,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"e0a61a71-eb96-4723-fed1-1d3f1452ec70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Training data prepared! Total samples: 279, Feature dimension: 128\n","✅ Scaler saved as /content/drive/MyDrive/Jewelify/Trained Features/scaler.pkl\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 239ms/step - loss: 3.5962 - mae: 1.8567 - mse: 3.5962 - val_loss: 3.5952 - val_mae: 1.8736 - val_mse: 3.5952\n","Epoch 2/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.6522 - mae: 1.8749 - mse: 3.6522 - val_loss: 3.5900 - val_mae: 1.8722 - val_mse: 3.5900\n","Epoch 3/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6809 - mae: 1.8894 - mse: 3.6809 - val_loss: 3.5847 - val_mae: 1.8708 - val_mse: 3.5847\n","Epoch 4/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6669 - mae: 1.8865 - mse: 3.6669 - val_loss: 3.5795 - val_mae: 1.8694 - val_mse: 3.5795\n","Epoch 5/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5978 - mae: 1.8661 - mse: 3.5978 - val_loss: 3.5743 - val_mae: 1.8680 - val_mse: 3.5743\n","Epoch 6/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6531 - mae: 1.8849 - mse: 3.6531 - val_loss: 3.5691 - val_mae: 1.8666 - val_mse: 3.5691\n","Epoch 7/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6230 - mae: 1.8673 - mse: 3.6230 - val_loss: 3.5638 - val_mae: 1.8652 - val_mse: 3.5638\n","Epoch 8/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5984 - mae: 1.8631 - mse: 3.5984 - val_loss: 3.5587 - val_mae: 1.8638 - val_mse: 3.5587\n","Epoch 9/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5332 - mae: 1.8480 - mse: 3.5332 - val_loss: 3.5535 - val_mae: 1.8624 - val_mse: 3.5535\n","Epoch 10/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6207 - mae: 1.8760 - mse: 3.6207 - val_loss: 3.5483 - val_mae: 1.8610 - val_mse: 3.5483\n","Epoch 11/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6319 - mae: 1.8806 - mse: 3.6319 - val_loss: 3.5431 - val_mae: 1.8596 - val_mse: 3.5431\n","Epoch 12/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4973 - mae: 1.8351 - mse: 3.4973 - val_loss: 3.5379 - val_mae: 1.8582 - val_mse: 3.5379\n","Epoch 13/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5748 - mae: 1.8570 - mse: 3.5748 - val_loss: 3.5327 - val_mae: 1.8569 - val_mse: 3.5327\n","Epoch 14/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5290 - mae: 1.8446 - mse: 3.5290 - val_loss: 3.5275 - val_mae: 1.8555 - val_mse: 3.5275\n","Epoch 15/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5720 - mae: 1.8624 - mse: 3.5720 - val_loss: 3.5224 - val_mae: 1.8541 - val_mse: 3.5224\n","Epoch 16/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5765 - mae: 1.8637 - mse: 3.5765 - val_loss: 3.5172 - val_mae: 1.8527 - val_mse: 3.5172\n","Epoch 17/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6260 - mae: 1.8790 - mse: 3.6260 - val_loss: 3.5121 - val_mae: 1.8513 - val_mse: 3.5121\n","Epoch 18/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5269 - mae: 1.8348 - mse: 3.5269 - val_loss: 3.5069 - val_mae: 1.8499 - val_mse: 3.5069\n","Epoch 19/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5537 - mae: 1.8579 - mse: 3.5537 - val_loss: 3.5018 - val_mae: 1.8485 - val_mse: 3.5018\n","Epoch 20/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4025 - mae: 1.8037 - mse: 3.4025 - val_loss: 3.4967 - val_mae: 1.8471 - val_mse: 3.4967\n","Epoch 21/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5035 - mae: 1.8401 - mse: 3.5035 - val_loss: 3.4915 - val_mae: 1.8457 - val_mse: 3.4915\n","Epoch 22/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5064 - mae: 1.8410 - mse: 3.5064 - val_loss: 3.4864 - val_mae: 1.8443 - val_mse: 3.4864\n","Epoch 23/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5762 - mae: 1.8622 - mse: 3.5762 - val_loss: 3.4813 - val_mae: 1.8429 - val_mse: 3.4813\n","Epoch 24/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5360 - mae: 1.8509 - mse: 3.5360 - val_loss: 3.4762 - val_mae: 1.8416 - val_mse: 3.4762\n","Epoch 25/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.5466 - mae: 1.8560 - mse: 3.5466 - val_loss: 3.4711 - val_mae: 1.8402 - val_mse: 3.4711\n","Epoch 26/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5194 - mae: 1.8517 - mse: 3.5194 - val_loss: 3.4659 - val_mae: 1.8388 - val_mse: 3.4659\n","Epoch 27/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5412 - mae: 1.8496 - mse: 3.5412 - val_loss: 3.4608 - val_mae: 1.8374 - val_mse: 3.4608\n","Epoch 28/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5362 - mae: 1.8459 - mse: 3.5362 - val_loss: 3.4558 - val_mae: 1.8360 - val_mse: 3.4558\n","Epoch 29/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4212 - mae: 1.8127 - mse: 3.4212 - val_loss: 3.4507 - val_mae: 1.8346 - val_mse: 3.4507\n","Epoch 30/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5484 - mae: 1.8587 - mse: 3.5484 - val_loss: 3.4456 - val_mae: 1.8332 - val_mse: 3.4456\n","Epoch 31/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4860 - mae: 1.8402 - mse: 3.4860 - val_loss: 3.4405 - val_mae: 1.8319 - val_mse: 3.4405\n","Epoch 32/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5163 - mae: 1.8499 - mse: 3.5163 - val_loss: 3.4355 - val_mae: 1.8305 - val_mse: 3.4355\n","Epoch 33/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4949 - mae: 1.8427 - mse: 3.4949 - val_loss: 3.4304 - val_mae: 1.8291 - val_mse: 3.4304\n","Epoch 34/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.4643 - mae: 1.8297 - mse: 3.4643 - val_loss: 3.4253 - val_mae: 1.8277 - val_mse: 3.4253\n","Epoch 35/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4349 - mae: 1.8248 - mse: 3.4349 - val_loss: 3.4203 - val_mae: 1.8263 - val_mse: 3.4203\n","Epoch 36/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4544 - mae: 1.8284 - mse: 3.4544 - val_loss: 3.4152 - val_mae: 1.8249 - val_mse: 3.4152\n","Epoch 37/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4669 - mae: 1.8281 - mse: 3.4669 - val_loss: 3.4102 - val_mae: 1.8236 - val_mse: 3.4102\n","Epoch 38/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5095 - mae: 1.8472 - mse: 3.5095 - val_loss: 3.4052 - val_mae: 1.8222 - val_mse: 3.4052\n","Epoch 39/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4274 - mae: 1.8255 - mse: 3.4274 - val_loss: 3.4001 - val_mae: 1.8208 - val_mse: 3.4001\n","Epoch 40/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.3736 - mae: 1.7894 - mse: 3.3736 - val_loss: 3.3951 - val_mae: 1.8194 - val_mse: 3.3951\n","Epoch 41/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4158 - mae: 1.8090 - mse: 3.4158 - val_loss: 3.3901 - val_mae: 1.8180 - val_mse: 3.3901\n","Epoch 42/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.3040 - mae: 1.7707 - mse: 3.3040 - val_loss: 3.3851 - val_mae: 1.8167 - val_mse: 3.3851\n","Epoch 43/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5002 - mae: 1.8506 - mse: 3.5002 - val_loss: 3.3801 - val_mae: 1.8153 - val_mse: 3.3801\n","Epoch 44/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.4270 - mae: 1.8199 - mse: 3.4270 - val_loss: 3.3751 - val_mae: 1.8139 - val_mse: 3.3751\n","Epoch 45/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4408 - mae: 1.8268 - mse: 3.4408 - val_loss: 3.3701 - val_mae: 1.8125 - val_mse: 3.3701\n","Epoch 46/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.4450 - mae: 1.8293 - mse: 3.4450 - val_loss: 3.3651 - val_mae: 1.8111 - val_mse: 3.3651\n","Epoch 47/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4144 - mae: 1.8146 - mse: 3.4144 - val_loss: 3.3601 - val_mae: 1.8098 - val_mse: 3.3601\n","Epoch 48/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.4040 - mae: 1.8157 - mse: 3.4040 - val_loss: 3.3551 - val_mae: 1.8084 - val_mse: 3.3551\n","Epoch 49/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.3953 - mae: 1.8166 - mse: 3.3953 - val_loss: 3.3501 - val_mae: 1.8070 - val_mse: 3.3501\n","Epoch 50/50\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.4311 - mae: 1.8255 - mse: 3.4311 - val_loss: 3.3452 - val_mae: 1.8056 - val_mse: 3.3452\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step\n","\n","🔍 **Model Performance** 🔍\n","Mean Absolute Error (MAE): 1.8056\n","Mean Squared Error (MSE): 3.3452\n","Root Mean Squared Error (RMSE): 1.8290\n","R-squared (R²): -38.4377\n","\n","✅ Model saved as /content/drive/MyDrive/Jewelify/Trained Features/keras/FNN_multi_model_feature_model.keras\n"]}]},{"cell_type":"markdown","source":["# **Below two cells are predicting compability and recomanding in seperate cells**"],"metadata":{"id":"19N3zsNflXsV"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model, Model\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","class JewelryCompatibilityPredictor:\n","    def __init__(self, model_path, scaler_path=None):\n","        \"\"\"Initialize the model and feature extractor.\"\"\"\n","        self.model = load_model(model_path)\n","        self.img_size = (224, 224)\n","        self.device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","        self.feature_size = 128  # Matches the training feature size\n","\n","        # Load or initialize scaler\n","        if scaler_path and os.path.exists(scaler_path):\n","            with open(scaler_path, 'rb') as f:\n","                self.scaler = pickle.load(f)\n","        else:\n","            self.scaler = StandardScaler()\n","            print(\"⚠️ No scaler provided; initializing a new one (results may vary unless fitted).\")\n","\n","        # Feature extractor: MobileNetV2 with size reduction to 128\n","        base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","        global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n","        reduction_layer = tf.keras.layers.Dense(self.feature_size, activation=\"relu\")\n","        self.feature_extractor = Model(\n","            inputs=base_model.input,\n","            outputs=reduction_layer(global_avg_layer(base_model.output))\n","        )\n","\n","    def extract_features(self, img_path):\n","        \"\"\"Extract features using MobileNetV2 reduced to 128 dimensions.\"\"\"\n","        if not os.path.exists(img_path):\n","            print(f\"❌ Error: Image file not found - {img_path}\")\n","            return None\n","\n","        try:\n","            img = image.load_img(img_path, target_size=self.img_size)\n","            img_array = image.img_to_array(img)\n","            img_array = np.expand_dims(img_array, axis=0)\n","            img_array = preprocess_input(img_array)\n","\n","            features = self.feature_extractor.predict(img_array, verbose=0)\n","            return features.reshape(1, -1)  # Shape: (1, 128)\n","        except Exception as e:\n","            print(f\"⚠️ Error extracting features from {img_path}: {e}\")\n","            return None\n","\n","    def categorize_suitability(self, score):\n","        \"\"\"Convert numerical suitability score into categories.\"\"\"\n","        if score < 0.5:\n","            return \"❌ Very Bad\"\n","        elif 0.5 <= score < 1.0:\n","            return \"⚠️ Bad\"\n","        elif 1.0 <= score < 1.5:\n","            return \"😐 Neutral\"\n","        elif 1.5 <= score < 2.0:\n","            return \"✅ Good\"\n","        else:\n","            return \"🌟 Very Good\"\n","\n","    def predict_compatibility(self, face_img_path=None, jewelry_img_path=None):\n","        \"\"\"Predict compatibility score for a given face and/or jewelry image.\"\"\"\n","        face_features = self.extract_features(face_img_path) if face_img_path else np.zeros((1, self.feature_size))\n","        jewelry_features = self.extract_features(jewelry_img_path) if jewelry_img_path else np.zeros((1, self.feature_size))\n","\n","        if face_features is None or jewelry_features is None:\n","            print(\"❌ Error: Could not extract features for provided image(s).\")\n","            return None\n","\n","        combined_features = np.concatenate((face_features, jewelry_features), axis=1)  # Shape: (1, 256)\n","        expected_input_shape = self.model.input_shape[1]\n","\n","        if combined_features.shape[1] != expected_input_shape:\n","            print(f\"❌ Shape Mismatch! Model expects {expected_input_shape}, but got {combined_features.shape[1]}\")\n","            return None\n","\n","        # Normalize features using the scaler from training\n","        combined_features = self.scaler.transform(combined_features)\n","\n","        with tf.device(self.device):\n","            prediction = self.model.predict(combined_features, verbose=0)[0][0]\n","\n","        category = self.categorize_suitability(prediction)\n","        print(f\"\\n🔮 **Predicted Suitability Score**: {prediction:.2f} ({category})\")\n","        return prediction, category\n","\n","# Usage\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/FNN_multi_model_feature_model.keras\"\n","scaler_path = \"/content/drive/MyDrive/Jewelify/Trained Features/scaler.pkl\"  # Save this during training\n","\n","# Save the scaler during training (add this to the training code after fitting)\n","# with open(scaler_path, 'wb') as f:\n","#     pickle.dump(scaler, f)\n","\n","predictor = JewelryCompatibilityPredictor(model_path, scaler_path)\n","\n","# Example predictions\n","# predictor.predict_compatibility(face_img_path=\"path/to/face.jpg\", jewelry_img_path=\"path/to/jewelry.jpg\")\n","# predictor.predict_compatibility(jewelry_img_path=\"path/to/jewelry.jpg\")  # Only jewelry"],"metadata":{"id":"F_mPowbHdGOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model, Model\n","import pickle\n","\n","def recommend_jewelry_from_pairwise(face_img_path, model_path, scaler_path, pairwise_features_path, min_top_n=10, max_top_n=15):\n","    \"\"\"\n","    Recommend top 10 to 15 jewelry options from pairwise_features.npy that suit the given face photo.\n","    Image names are sorted alphabetically within the top recommendations.\n","\n","    Args:\n","        face_img_path (str): Path to the face image (guaranteed to exist from JewelryCompatibilityPredictor).\n","        model_path (str): Path to the trained model (.keras file).\n","        scaler_path (str): Path to the saved StandardScaler (.pkl file).\n","        pairwise_features_path (str): Path to pairwise_features.npy.\n","        min_top_n (int): Minimum number of recommendations (default: 10).\n","        max_top_n (int): Maximum number of recommendations (default: 15).\n","\n","    Returns:\n","        list: Top 10-15 (image_name, score) pairs, with image names sorted alphabetically.\n","    \"\"\"\n","    # Load model and scaler\n","    model = load_model(model_path)\n","    with open(scaler_path, 'rb') as f:\n","        scaler = pickle.load(f)\n","\n","    # Initialize feature extractor (MobileNetV2 reduced to 128 dimensions)\n","    img_size = (224, 224)\n","    feature_size = 128\n","    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","    global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n","    reduction_layer = tf.keras.layers.Dense(feature_size, activation=\"relu\")\n","    feature_extractor = Model(\n","        inputs=base_model.input,\n","        outputs=reduction_layer(global_avg_layer(base_model.output))\n","    )\n","\n","    # Extract face features (face_img_path is guaranteed to exist)\n","    try:\n","        img = image.load_img(face_img_path, target_size=img_size)\n","        img_array = image.img_to_array(img)\n","        img_array = np.expand_dims(img_array, axis=0)\n","        img_array = preprocess_input(img_array)\n","        face_features = feature_extractor.predict(img_array, verbose=0).reshape(1, -1)  # Shape: (1, 128)\n","    except Exception as e:\n","        print(f\"⚠️ Error extracting features from {face_img_path}: {e}\")\n","        return None\n","\n","    # Load pairwise features (necklace + earrings)\n","    try:\n","        pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","        pairwise_features = {k: np.array(v) for k, v in pairwise_features.items()}\n","    except FileNotFoundError as e:\n","        print(f\"❌ Error loading pairwise features: {e}\")\n","        return None\n","\n","    # Check feature size consistency\n","    expected_input_shape = model.input_shape[1]  # Should be 256 (128 face + 128 jewelry)\n","    if face_features.shape[1] != feature_size:\n","        print(f\"❌ Face feature size mismatch! Expected {feature_size}, got {face_features.shape[1]}\")\n","        return None\n","\n","    # Predict compatibility for each pairwise feature\n","    recommendations = []\n","    for image_name, jewelry_features in pairwise_features.items():\n","        if jewelry_features.shape[0] != feature_size:\n","            print(f\"⚠️ Skipping {image_name}: Jewelry feature size mismatch (expected {feature_size}, got {jewelry_features.shape[0]})\")\n","            continue\n","\n","        # Concatenate face and jewelry features\n","        combined_features = np.concatenate((face_features, jewelry_features.reshape(1, -1)), axis=1)  # Shape: (1, 256)\n","        if combined_features.shape[1] != expected_input_shape:\n","            print(f\"❌ Shape mismatch for {image_name}! Expected {expected_input_shape}, got {combined_features.shape[1]}\")\n","            continue\n","\n","        # Normalize and predict\n","        combined_features = scaler.transform(combined_features)\n","        score = model.predict(combined_features, verbose=0)[0][0]\n","        recommendations.append((image_name, score))\n","\n","    # Sort by score (descending) and determine number of recommendations\n","    recommendations.sort(key=lambda x: x[1], reverse=True)\n","    total_options = len(recommendations)\n","    top_n = min(max(min_top_n, total_options), max_top_n)  # Between 10 and 15, capped by available options\n","    top_recommendations = recommendations[:top_n]\n","\n","    # Sort top recommendations alphabetically by image name\n","    top_recommendations.sort(key=lambda x: x[0])  # Sort by image_name\n","\n","    # Print results\n","    print(f\"\\n🔮 **Top {top_n} Jewelry Recommendations for {face_img_path}**\")\n","    for i, (img_name, score) in enumerate(top_recommendations, 1):\n","        print(f\"{i}. {img_name}: Score = {score:.2f}\")\n","\n","    return top_recommendations\n","\n","# Usage example (assuming JewelryCompatibilityPredictor has run)\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/multi_model_feature_model.keras\"\n","scaler_path = \"/content/drive/MyDrive/Jewelify/Trained Features/scaler.pkl\"\n","pairwise_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/pairwise_features.npy\"\n","face_img_path = \"path/to/your/face.jpg\"  # Provided by JewelryCompatibilityPredictor\n","\n","top_recommendations = recommend_jewelry_from_pairwise(\n","    face_img_path=face_img_path,\n","    model_path=model_path,\n","    scaler_path=scaler_path,\n","    pairwise_features_path=pairwise_features_path,\n","    min_top_n=10,\n","    max_top_n=15\n",")"],"metadata":{"id":"5ShADDLddF-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Below code is FNN Predicting and recommanding in single file**"],"metadata":{"id":"YtCLSOo3ls_5"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model, Model\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","class JewelryCompatibilityPredictor:\n","    def __init__(self, model_path, scaler_path, pairwise_features_path=None):\n","        \"\"\"Initialize the model, feature extractor, and optional pairwise features.\"\"\"\n","        self.model = load_model(model_path)\n","        self.img_size = (224, 224)\n","        self.device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","        self.feature_size = 128  # Fixed feature size for the model\n","\n","        # Load scaler\n","        if scaler_path and os.path.exists(scaler_path):\n","            with open(scaler_path, 'rb') as f:\n","                self.scaler = pickle.load(f)\n","        else:\n","            self.scaler = StandardScaler()\n","            print(\"⚠️ No scaler provided; initializing a new one (results may vary unless fitted).\")\n","\n","        # Feature extractor: MobileNetV2 with size reduction to 128\n","        base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","        global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n","        reduction_layer = tf.keras.layers.Dense(self.feature_size, activation=\"relu\")\n","        self.feature_extractor = Model(\n","            inputs=base_model.input,\n","            outputs=reduction_layer(global_avg_layer(base_model.output))\n","        )\n","\n","        # Load and adjust pairwise features if provided\n","        self.pairwise_features_path = pairwise_features_path\n","        self.pairwise_features = None\n","        if pairwise_features_path and os.path.exists(pairwise_features_path):\n","            try:\n","                pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","                self.pairwise_features = {}\n","                for k, v in pairwise_features.items():\n","                    v = np.array(v)\n","                    if v.shape[0] != self.feature_size:  # If not 128, reduce it\n","                        v = v.reshape(1, -1)\n","                        v = tf.keras.layers.Dense(self.feature_size, activation=\"relu\")(v).numpy()\n","                    self.pairwise_features[k] = v.reshape(-1)\n","            except Exception as e:\n","                print(f\"❌ Error loading or processing pairwise features: {e}\")\n","\n","    def extract_features(self, img_path):\n","        \"\"\"Extract features using MobileNetV2 reduced to 128 dimensions.\"\"\"\n","        if not os.path.exists(img_path):\n","            print(f\"❌ Error: Image file not found - {img_path}\")\n","            return None\n","\n","        try:\n","            img = image.load_img(img_path, target_size=self.img_size)\n","            img_array = image.img_to_array(img)\n","            img_array = np.expand_dims(img_array, axis=0)\n","            img_array = preprocess_input(img_array)\n","\n","            features = self.feature_extractor.predict(img_array, verbose=0)\n","            return features.reshape(1, -1)  # Shape: (1, 128)\n","        except Exception as e:\n","            print(f\"⚠️ Error extracting features from {img_path}: {e}\")\n","            return None\n","\n","    def categorize_suitability(self, score):\n","        \"\"\"Convert numerical suitability score into categories.\"\"\"\n","        if score < 0.5:\n","            return \"❌ Very Bad\"\n","        elif 0.5 <= score < 1.0:\n","            return \"⚠️ Bad\"\n","        elif 1.0 <= score < 1.5:\n","            return \"😐 Neutral\"\n","        elif 1.5 <= score < 2.0:\n","            return \"✅ Good\"\n","        else:\n","            return \"🌟 Very Good\"\n","\n","    def predict_compatibility(self, face_img_path=None, jewelry_img_path=None):\n","        \"\"\"Predict compatibility score using only jewelry features (fixed 128 dimensions).\"\"\"\n","        features = self.extract_features(jewelry_img_path) if jewelry_img_path else self.extract_features(face_img_path)\n","\n","        if features is None:\n","            print(\"❌ Error: Could not extract features for provided image(s).\")\n","            return None, None, None\n","\n","        expected_input_shape = 128  # Fixed to match model expectation\n","        if features.shape[1] != expected_input_shape:\n","            print(f\"❌ Shape Mismatch! Model expects {expected_input_shape}, but got {features.shape[1]}\")\n","            return None, None, None\n","\n","        features = self.scaler.transform(features)\n","\n","        with tf.device(self.device):\n","            prediction = self.model.predict(features, verbose=0)[0][0]\n","\n","        category = self.categorize_suitability(prediction)\n","        print(f\"\\n🔮 **Predicted Suitability Score**: {prediction:.2f} ({category})\")\n","\n","        if face_img_path and self.pairwise_features:\n","            face_features = self.extract_features(face_img_path)\n","            if face_features is not None:\n","                recommendations = self.recommend_jewelry(face_features, min_top_n=10, max_top_n=15)\n","                return prediction, category, recommendations\n","\n","        return prediction, category, None\n","\n","    def recommend_jewelry(self, face_features, min_top_n=10, max_top_n=15):\n","        \"\"\"Recommend top 10-15 jewelry options from pairwise_features (fixed 128 dimensions).\"\"\"\n","        if not self.pairwise_features:\n","            print(\"❌ No pairwise features loaded for recommendations.\")\n","            return None\n","\n","        expected_input_shape = 128\n","        if face_features.shape[1] != self.feature_size:\n","            print(f\"❌ Face feature size mismatch! Expected {self.feature_size}, got {face_features.shape[1]}\")\n","            return None\n","\n","        recommendations = []\n","        for image_name, jewelry_features in self.pairwise_features.items():\n","            if jewelry_features.shape[0] != self.feature_size:\n","                print(f\"⚠️ Skipping {image_name}: Jewelry feature size mismatch (expected {self.feature_size}, got {jewelry_features.shape[0]})\")\n","                continue\n","\n","            jewelry_features = jewelry_features.reshape(1, -1)\n","            if jewelry_features.shape[1] != expected_input_shape:\n","                print(f\"❌ Shape mismatch for {image_name}! Expected {expected_input_shape}, got {jewelry_features.shape[1]}\")\n","                continue\n","\n","            normalized_features = self.scaler.transform(jewelry_features)\n","            score = self.model.predict(normalized_features, verbose=0)[0][0]\n","            recommendations.append((image_name, score))\n","\n","        recommendations.sort(key=lambda x: x[1], reverse=True)\n","        total_options = len(recommendations)\n","        top_n = min(max(min_top_n, total_options), max_top_n)\n","        top_recommendations = recommendations[:top_n]\n","        top_recommendations.sort(key=lambda x: x[0])\n","\n","        print(f\"\\n🔮 **Top {top_n} Recommended Jewelry Image Names**\")\n","        for img_name, _ in top_recommendations:\n","            print(img_name)\n","\n","        return [img_name for img_name, _ in top_recommendations]\n","\n","# Usage\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/multi_model_feature_model.keras\"\n","scaler_path = \"/content/drive/MyDrive/Jewelify/Trained Features/FNN_scaler.pkl\"\n","pairwise_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/pairwise_features.npy\"\n","\n","# predictor = JewelryCompatibilityPredictor(model_path, scaler_path, pairwise_features_path)\n","\n","# # Test with your images\n","# prediction, category, recommendations = predictor.predict_compatibility(\n","#     face_img_path=\"/content/girl_7.jpg\",\n","#     jewelry_img_path=\"/content/download.jpg\"\n","# )"],"metadata":{"id":"DlzFSEtXl2TX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Predict Compatibility + Get Recommendations**"],"metadata":{"id":"0mUU5YLbmeTY"}},{"cell_type":"code","source":["predictor = JewelryCompatibilityPredictor(model_path, scaler_path, pairwise_features_path)\n","\n","# Test with your images\n","prediction, category, recommendations = predictor.predict_compatibility(\n","    face_img_path=\"/content/girl_7.jpg\",\n","    jewelry_img_path=\"/content/download.jpg\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6v8VdLv-mdEv","executionInfo":{"status":"ok","timestamp":1740500816091,"user_tz":-330,"elapsed":23648,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"fa6ad54e-0703-4cdd-944c-a266301d6783"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔮 **Predicted Suitability Score**: 0.17 (❌ Very Bad)\n","\n","🔮 **Top 15 Recommended Jewelry Image Names**\n","Necklace with earings_101.jpg\n","Necklace with earings_11.jpg\n","Necklace with earings_150.jpg\n","Necklace with earings_157.jpg\n","Necklace with earings_159.jpg\n","Necklace with earings_188.jpg\n","Necklace with earings_190.jpg\n","Necklace with earings_201.jpg\n","Necklace with earings_202.jpg\n","Necklace with earings_213.jpg\n","Necklace with earings_234.jpg\n","Necklace with earings_5.jpg\n","Necklace with earings_63.jpg\n","Necklace with earings_67.jpg\n","Necklace with earings_89.jpg\n"]}]},{"cell_type":"markdown","source":["**Predict Compatibility Only**"],"metadata":{"id":"MZpp4HpbmnwN"}},{"cell_type":"code","source":["# predictor = JewelryCompatibilityPredictor(model_path, scaler_path)  # No pairwise_features_path\n","# prediction, category = predictor.predict_compatibility(\n","#     face_img_path=\"path/to/face.jpg\",\n","#     jewelry_img_path=\"path/to/jewelry.jpg\"  # Optional\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"coY5L7KfmoLJ","executionInfo":{"status":"error","timestamp":1740500816890,"user_tz":-330,"elapsed":797,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"66042ac8-7a0a-4c72-e99c-81c0453f93d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["❌ Error: Image file not found - path/to/jewelry.jpg\n","❌ Error: Could not extract features for provided image(s).\n"]},{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e47d249960b6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJewelryCompatibilityPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No pairwise_features_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m prediction, category = predictor.predict_compatibility(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mface_img_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"path/to/face.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjewelry_img_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"path/to/jewelry.jpg\"\u001b[0m  \u001b[0;31m# Optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FuyhUTvewsfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Reinforcement Learning**"],"metadata":{"id":"DXpLrqIGZ1YN"}},{"cell_type":"markdown","source":["**Training the model**"],"metadata":{"id":"-0MVYWmxxGLR"}},{"cell_type":"code","source":["# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Dense, Input\n","# from tensorflow.keras.optimizers import Adam\n","# import random\n","# from collections import deque\n","# import pickle\n","# import os\n","\n","# # Paths\n","# base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","# multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","# pairwise_features_path = f\"{base_path}/pandas/pairwise_features.npy\"\n","# model_save_path = f\"{base_path}/keras/rl_jewelry_model.keras\"\n","# scaler_save_path = f\"{base_path}/scaler.pkl\"\n","\n","# # RL Agent\n","# class JewelryRLAgent:\n","#     def __init__(self, state_size, action_size):\n","#         self.state_size = state_size  # Updated to 1280\n","#         self.action_size = action_size  # Number of jewelry items\n","#         self.memory = deque(maxlen=2000)\n","#         self.gamma = 0.95  # Discount factor\n","#         self.epsilon = 1.0  # Exploration rate\n","#         self.epsilon_min = 0.01\n","#         self.epsilon_decay = 0.995\n","#         self.learning_rate = 0.001\n","#         self.model = self._build_model()\n","\n","#     def _build_model(self):\n","#         \"\"\"Q-Network for Deep Q-Learning with Input layer.\"\"\"\n","#         model = Sequential([\n","#             Input(shape=(self.state_size,)),\n","#             Dense(64, activation='relu'),\n","#             Dense(32, activation='relu'),\n","#             Dense(self.action_size, activation='linear')  # Q-values for each action\n","#         ])\n","#         model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","#         return model\n","\n","#     def remember(self, state, action, reward, next_state, done):\n","#         \"\"\"Store experience in memory.\"\"\"\n","#         self.memory.append((state, action, reward, next_state, done))\n","\n","#     def act(self, state):\n","#         \"\"\"Choose an action (jewelry item).\"\"\"\n","#         if np.random.rand() <= self.epsilon:\n","#             return random.randrange(self.action_size)  # Explore\n","#         q_values = self.model.predict(state, verbose=0)\n","#         return np.argmax(q_values[0])  # Exploit\n","\n","#     def replay(self, batch_size):\n","#         \"\"\"Train the model using experience replay.\"\"\"\n","#         minibatch = random.sample(self.memory, batch_size)\n","#         for state, action, reward, next_state, done in minibatch:\n","#             target = reward\n","#             if not done:\n","#                 target = reward + self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n","#             target_f = self.model.predict(state, verbose=0)\n","#             target_f[0][action] = target\n","#             self.model.fit(state, target_f, epochs=1, verbose=0)\n","#         if self.epsilon > self.epsilon_min:\n","#             self.epsilon *= self.epsilon_decay\n","\n","# # Simulated Environment\n","# class JewelryEnvironment:\n","#     def __init__(self, jewelry_features):\n","#         self.jewelry_features = [f for f in jewelry_features.values() if f is not None and f.shape[0] == 1280]\n","#         self.action_size = len(self.jewelry_features)\n","#         self.state = None\n","\n","#     def reset(self, face_features):\n","#         \"\"\"Reset environment with a new face.\"\"\"\n","#         if face_features is None or face_features.shape[1] != 1280:\n","#             print(\"⚠️ Invalid face features, returning zeros\")\n","#             return np.zeros((1, 1280))\n","#         self.state = face_features.reshape(1, -1)\n","#         return self.state\n","\n","#     def step(self, action):\n","#         \"\"\"Take an action (select jewelry) and return reward, next state, done.\"\"\"\n","#         if action >= self.action_size:\n","#             print(f\"⚠️ Invalid action {action}, defaulting to 0\")\n","#             action = 0\n","#         selected_jewelry = self.jewelry_features[action]\n","#         reward = self._simulate_reward(self.state, selected_jewelry)\n","#         next_state = self.state  # State doesn’t change\n","#         done = True  # One-step recommendation\n","#         return next_state, reward, done\n","\n","#     def _simulate_reward(self, face_features, jewelry_features):\n","#         \"\"\"Simulate reward (e.g., cosine similarity).\"\"\"\n","#         try:\n","#             face_features = face_features.reshape(-1)\n","#             jewelry_features = jewelry_features.reshape(-1)\n","#             if face_features.size != 1280 or jewelry_features.size != 1280:\n","#                 raise ValueError(\"Feature size mismatch\")\n","#             cos_sim = np.dot(face_features, jewelry_features) / (np.linalg.norm(face_features) * np.linalg.norm(jewelry_features))\n","#             return float(cos_sim)  # Reward between -1 and 1\n","#         except (ValueError, TypeError) as e:\n","#             print(f\"⚠️ Error computing reward: {e}, returning 0\")\n","#             return 0.0\n","\n","# # Load Features and Train\n","# def train_rl_model():\n","#     # Load features\n","#     try:\n","#         multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","#         pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","#     except Exception as e:\n","#         print(f\"❌ Error loading features: {e}\")\n","#         return\n","\n","#     face_features = [np.array(f) for f in multi_object_features.values() if f is not None and f.size == 1280]\n","#     jewelry_features = pairwise_features\n","\n","#     if not face_features or not jewelry_features:\n","#         print(\"❌ No valid features found, aborting training\")\n","#         print(f\"Found face features: {len(face_features)}, jewelry features: {len(jewelry_features)}\")\n","#         return\n","\n","#     state_size = 1280  # Updated to match your features\n","#     action_size = len(jewelry_features)\n","\n","#     # Initialize agent and environment\n","#     agent = JewelryRLAgent(state_size, action_size)\n","#     env = JewelryEnvironment(jewelry_features)\n","#     scaler = StandardScaler()\n","\n","#     # Normalize features\n","#     all_features = np.vstack([f for f in face_features] + [f for f in jewelry_features.values() if f is not None and f.size == 1280])\n","#     if all_features.size == 0:\n","#         print(\"❌ No valid features for scaling, aborting\")\n","#         return\n","#     scaler.fit(all_features)\n","#     face_features = [scaler.transform(f.reshape(1, -1)) for f in face_features if f is not None]\n","\n","#     # Training loop\n","#     episodes = 1000\n","#     batch_size = 32\n","#     for e in range(episodes):\n","#         face_idx = random.randrange(len(face_features))\n","#         state = env.reset(face_features[face_idx])\n","#         action = agent.act(state)\n","#         next_state, reward, done = env.step(action)\n","\n","#         agent.remember(state, action, reward, next_state, done)\n","#         if len(agent.memory) > batch_size:\n","#             agent.replay(batch_size)\n","\n","#         if e % 50 == 0:\n","#             print(f\"Episode {e}/{episodes}, Epsilon: {agent.epsilon:.2f}, Reward: {reward:.2f}\")\n","\n","#     # Save model and scaler\n","#     agent.model.save(model_save_path)\n","#     with open(scaler_save_path, 'wb') as f:\n","#         pickle.dump(scaler, f)\n","#     print(f\"✅ Model saved to {model_save_path}, Scaler saved to {scaler_save_path}\")\n","\n","# if __name__ == \"__main__\":\n","#     train_rl_model()"],"metadata":{"id":"ke4CT9L9wuBX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code is training the code pararelly and made using the grok"],"metadata":{"id":"m-b6JFyC_yvc"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","import random\n","from collections import deque\n","import pickle\n","import os\n","from sklearn.preprocessing import StandardScaler  # Added missing import\n","\n","# Ensure GPU is available\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    print(\"✅ GPU available:\", physical_devices)\n","else:\n","    print(\"⚠️ No GPU found, running on CPU\")\n","\n","# Paths\n","base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","pairwise_features_path = f\"{base_path}/pandas/pairwise_features.npy\"\n","model_save_path = f\"{base_path}/keras/rl_jewelry_model.keras\"\n","scaler_save_path = f\"{base_path}/rl_scaler.pkl\"\n","\n","# RL Agent\n","class JewelryRLAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size  # 1280\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=5000)  # Increased memory size\n","        self.gamma = 0.99  # Increased for long-term reward focus\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.999  # Slower decay for more exploration\n","        self.learning_rate = 0.0001  # Reduced for stability\n","        self.device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","        self.model = self._build_model()\n","\n","    def _build_model(self):\n","        \"\"\"Q-Network for Deep Q-Learning.\"\"\"\n","        model = Sequential([\n","            Input(shape=(self.state_size,)),\n","            Dense(128, activation='relu'),  # Increased capacity\n","            Dense(64, activation='relu'),\n","            Dense(self.action_size, activation='linear')\n","        ])\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        \"\"\"Store experience in memory.\"\"\"\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, states):\n","        \"\"\"Choose actions for a batch of states.\"\"\"\n","        if np.random.rand() <= self.epsilon:\n","            return np.random.randint(0, self.action_size, size=states.shape[0])\n","        with tf.device(self.device):\n","            q_values = self.model.predict(states, verbose=0)\n","        return np.argmax(q_values, axis=1)\n","\n","    def replay(self, batch_size):\n","        \"\"\"Train the model using batched experience replay on GPU.\"\"\"\n","        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n","        states, actions, rewards, next_states, dones = zip(*minibatch)\n","\n","        states = np.vstack(states)\n","        next_states = np.vstack(next_states)\n","        actions = np.array(actions)\n","        rewards = np.array(rewards)\n","        dones = np.array(dones)\n","\n","        with tf.device(self.device):\n","            next_q_values = self.model.predict(next_states, verbose=0)\n","            target_q_values = self.model.predict(states, verbose=0)\n","            max_next_q = np.max(next_q_values, axis=1)\n","            targets = rewards + self.gamma * max_next_q * (1 - dones)\n","            target_q_values[range(len(actions)), actions] = targets\n","\n","            self.model.fit(states, target_q_values, epochs=1, verbose=0)\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","# Vectorized Environment\n","class JewelryEnvironment:\n","    def __init__(self, jewelry_features):\n","        self.jewelry_features = [f for f in jewelry_features.values() if f is not None and f.shape[0] == 1280]\n","        self.action_size = len(self.jewelry_features)\n","\n","    def reset(self, face_features_batch):\n","        \"\"\"Reset environment with a batch of face features.\"\"\"\n","        if face_features_batch is None or face_features_batch.shape[-1] != 1280:\n","            print(\"⚠️ Invalid face features, returning zeros\")\n","            return np.zeros((face_features_batch.shape[0], 1280))\n","        return face_features_batch\n","\n","    def step(self, actions):\n","        \"\"\"Take actions for a batch and return rewards, next states, dones.\"\"\"\n","        batch_size = len(actions)\n","        selected_jewelry = [self.jewelry_features[a] for a in actions]\n","        rewards = np.array([self._simulate_reward(self.state[i], selected_jewelry[i]) for i in range(batch_size)])\n","        next_states = self.state.copy()\n","        dones = np.ones(batch_size, dtype=bool)\n","        return next_states, rewards, dones\n","\n","    def _simulate_reward(self, face_features, jewelry_features):\n","        \"\"\"Simulate reward (scaled cosine similarity).\"\"\"\n","        try:\n","            face_features = face_features.reshape(-1)\n","            jewelry_features = jewelry_features.reshape(-1)\n","            if face_features.size != 1280 or jewelry_features.size != 1280:\n","                raise ValueError(\"Feature size mismatch\")\n","            cos_sim = np.dot(face_features, jewelry_features) / (np.linalg.norm(face_features) * np.linalg.norm(jewelry_features))\n","            # Scale to 0-1 range for positive rewards\n","            reward = (cos_sim + 1) / 2  # Maps -1..1 to 0..1\n","            return float(reward)\n","        except (ValueError, TypeError) as e:\n","            print(f\"⚠️ Error computing reward: {e}, returning 0.5\")\n","            return 0.5  # Neutral default reward\n","\n","# Load Features and Train\n","def train_rl_model():\n","    # Load features\n","    try:\n","        multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","        pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","    except Exception as e:\n","        print(f\"❌ Error loading features: {e}\")\n","        return\n","\n","    face_features = [np.array(f) for f in multi_object_features.values() if f is not None and f.size == 1280]\n","    jewelry_features = pairwise_features\n","\n","    if not face_features or not jewelry_features:\n","        print(\"❌ No valid features found, aborting training\")\n","        print(f\"Found face features: {len(face_features)}, jewelry features: {len(jewelry_features)}\")\n","        return\n","\n","    state_size = 1280\n","    action_size = len(jewelry_features)\n","\n","    # Initialize agent and environment\n","    agent = JewelryRLAgent(state_size, action_size)\n","    env = JewelryEnvironment(jewelry_features)\n","    scaler = StandardScaler()\n","\n","    # Normalize features\n","    all_features = np.vstack([f for f in face_features] + [f for f in jewelry_features.values() if f is not None and f.size == 1280])\n","    if all_features.size == 0:\n","        print(\"❌ No valid features for scaling, aborting\")\n","        return\n","    scaler.fit(all_features)\n","    face_features = np.vstack([scaler.transform(f.reshape(1, -1)) for f in face_features if f is not None])\n","\n","    # Training loop\n","    episodes = 2000  # Increased for more learning time\n","    batch_size = 32\n","    for e in range(episodes):\n","        indices = np.random.choice(len(face_features), batch_size, replace=True)\n","        state_batch = face_features[indices]\n","        env.state = state_batch\n","\n","        actions = agent.act(state_batch)\n","        next_states, rewards, dones = env.step(actions)\n","\n","        for i in range(batch_size):\n","            agent.remember(state_batch[i:i+1], actions[i], rewards[i], next_states[i:i+1], dones[i])\n","\n","        if len(agent.memory) > batch_size:\n","            agent.replay(batch_size)\n","\n","        if e % 50 == 0:\n","            avg_reward = np.mean(rewards)\n","            max_q = np.max(agent.model.predict(state_batch[:1], verbose=0))\n","            print(f\"Episode {e}/{episodes}, Epsilon: {agent.epsilon:.3f}, Avg Reward: {avg_reward:.3f}, Max Q-Value: {max_q:.3f}\")\n","\n","    # Save model and scaler\n","    agent.model.save(model_save_path)\n","    with open(scaler_save_path, 'wb') as f:\n","        pickle.dump(scaler, f)\n","    print(f\"✅ Model saved to {model_save_path}, Scaler saved to {scaler_save_path}\")\n","\n","if __name__ == \"__main__\":\n","    train_rl_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dC7ihpYW_cCc","executionInfo":{"status":"ok","timestamp":1740570553167,"user_tz":-330,"elapsed":796665,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"6977772f-4a51-4fe5-d9bf-29e02005617d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ No GPU found, running on CPU\n","Episode 0/2000, Epsilon: 1.000, Avg Reward: 0.470, Max Q-Value: 0.762\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 963 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a0eadeb3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Episode 50/2000, Epsilon: 0.951, Avg Reward: 0.462, Max Q-Value: 1.490\n","Episode 100/2000, Epsilon: 0.905, Avg Reward: 0.465, Max Q-Value: 0.969\n","Episode 150/2000, Epsilon: 0.861, Avg Reward: 0.471, Max Q-Value: 0.519\n","Episode 200/2000, Epsilon: 0.819, Avg Reward: 0.473, Max Q-Value: 0.689\n","Episode 250/2000, Epsilon: 0.779, Avg Reward: 0.482, Max Q-Value: 0.821\n","Episode 300/2000, Epsilon: 0.741, Avg Reward: 0.465, Max Q-Value: 0.600\n","Episode 350/2000, Epsilon: 0.705, Avg Reward: 0.466, Max Q-Value: 0.781\n","Episode 400/2000, Epsilon: 0.670, Avg Reward: 0.480, Max Q-Value: 0.682\n","Episode 450/2000, Epsilon: 0.637, Avg Reward: 0.477, Max Q-Value: 0.574\n","Episode 500/2000, Epsilon: 0.606, Avg Reward: 0.469, Max Q-Value: 0.586\n","Episode 550/2000, Epsilon: 0.577, Avg Reward: 0.466, Max Q-Value: 0.474\n","Episode 600/2000, Epsilon: 0.549, Avg Reward: 0.474, Max Q-Value: 0.578\n","Episode 650/2000, Epsilon: 0.522, Avg Reward: 0.466, Max Q-Value: 0.651\n","Episode 700/2000, Epsilon: 0.496, Avg Reward: 0.476, Max Q-Value: 0.552\n","Episode 750/2000, Epsilon: 0.472, Avg Reward: 0.468, Max Q-Value: 0.550\n","Episode 800/2000, Epsilon: 0.449, Avg Reward: 0.475, Max Q-Value: 0.568\n","Episode 850/2000, Epsilon: 0.427, Avg Reward: 0.469, Max Q-Value: 0.512\n","Episode 900/2000, Epsilon: 0.406, Avg Reward: 0.483, Max Q-Value: 0.550\n","Episode 950/2000, Epsilon: 0.387, Avg Reward: 0.462, Max Q-Value: 0.512\n","Episode 1000/2000, Epsilon: 0.368, Avg Reward: 0.465, Max Q-Value: 0.549\n","Episode 1050/2000, Epsilon: 0.350, Avg Reward: 0.464, Max Q-Value: 0.438\n","Episode 1100/2000, Epsilon: 0.333, Avg Reward: 0.466, Max Q-Value: 0.609\n","Episode 1150/2000, Epsilon: 0.316, Avg Reward: 0.482, Max Q-Value: 0.523\n","Episode 1200/2000, Epsilon: 0.301, Avg Reward: 0.475, Max Q-Value: 0.461\n","Episode 1250/2000, Epsilon: 0.286, Avg Reward: 0.474, Max Q-Value: 0.497\n","Episode 1300/2000, Epsilon: 0.272, Avg Reward: 0.472, Max Q-Value: 0.538\n","Episode 1350/2000, Epsilon: 0.259, Avg Reward: 0.489, Max Q-Value: 0.520\n","Episode 1400/2000, Epsilon: 0.246, Avg Reward: 0.470, Max Q-Value: 0.472\n","Episode 1450/2000, Epsilon: 0.234, Avg Reward: 0.467, Max Q-Value: 0.499\n","Episode 1500/2000, Epsilon: 0.223, Avg Reward: 0.476, Max Q-Value: 0.508\n","Episode 1550/2000, Epsilon: 0.212, Avg Reward: 0.477, Max Q-Value: 0.521\n","Episode 1600/2000, Epsilon: 0.202, Avg Reward: 0.485, Max Q-Value: 0.556\n","Episode 1650/2000, Epsilon: 0.192, Avg Reward: 0.482, Max Q-Value: 0.462\n","Episode 1700/2000, Epsilon: 0.183, Avg Reward: 0.469, Max Q-Value: 0.441\n","Episode 1750/2000, Epsilon: 0.174, Avg Reward: 0.470, Max Q-Value: 0.479\n","Episode 1800/2000, Epsilon: 0.165, Avg Reward: 0.472, Max Q-Value: 0.437\n","Episode 1850/2000, Epsilon: 0.157, Avg Reward: 0.474, Max Q-Value: 0.487\n","Episode 1900/2000, Epsilon: 0.149, Avg Reward: 0.477, Max Q-Value: 0.493\n","Episode 1950/2000, Epsilon: 0.142, Avg Reward: 0.475, Max Q-Value: 0.491\n","✅ Model saved to /content/drive/MyDrive/Jewelify/Trained Features/keras/rl_jewelry_model.keras, Scaler saved to /content/drive/MyDrive/Jewelify/Trained Features/scaler.pkl\n"]}]},{"cell_type":"markdown","source":["This code is made by the Chat Gpt"],"metadata":{"id":"UFhILLS4AAyO"}},{"cell_type":"code","source":["# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Dense, Input\n","# from tensorflow.keras.optimizers import Adam\n","# import random\n","# from collections import deque\n","# import pickle\n","# import os\n","\n","# # Enable GPU usage\n","# gpus = tf.config.list_physical_devices(\"GPU\")\n","# if gpus:\n","#     try:\n","#         for gpu in gpus:\n","#             tf.config.experimental.set_memory_growth(gpu, True)\n","#         print(\"✅ GPU detected and enabled\")\n","#     except RuntimeError as e:\n","#         print(f\"⚠️ Error enabling GPU: {e}\")\n","\n","# # Paths\n","# base_path = \"/content/drive/MyDrive/Jewelify/Trained Features\"\n","# multi_object_features_path = f\"{base_path}/pandas/multi_object_features.npy\"\n","# pairwise_features_path = f\"{base_path}/pandas/pairwise_features.npy\"\n","# model_save_path = f\"{base_path}/keras/rl_jewelry_model.keras\"\n","# scaler_save_path = f\"{base_path}/scaler.pkl\"\n","\n","# # RL Agent\n","# class JewelryRLAgent:\n","#     def __init__(self, state_size, action_size):\n","#         self.state_size = state_size\n","#         self.action_size = action_size\n","#         self.memory = deque(maxlen=2000)\n","#         self.gamma = 0.95\n","#         self.epsilon = 1.0\n","#         self.epsilon_min = 0.01\n","#         self.epsilon_decay = 0.995\n","#         self.learning_rate = 0.001\n","#         self.model = self._build_model()\n","\n","#     def _build_model(self):\n","#         \"\"\"Q-Network for Deep Q-Learning optimized for GPU\"\"\"\n","#         model = Sequential([\n","#             Input(shape=(self.state_size,)),\n","#             Dense(128, activation='relu'),  # Increased neurons for better feature learning\n","#             Dense(64, activation='relu'),\n","#             Dense(self.action_size, activation='linear')\n","#         ])\n","#         model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","#         return model\n","\n","#     def remember(self, state, action, reward, next_state, done):\n","#         \"\"\"Store experience in memory.\"\"\"\n","#         self.memory.append((state, action, reward, next_state, done))\n","\n","#     def act(self, state):\n","#         \"\"\"Choose an action (jewelry item)\"\"\"\n","#         if np.random.rand() <= self.epsilon:\n","#             return random.randrange(self.action_size)\n","#         q_values = self.model.predict(state, verbose=0)\n","#         return np.argmax(q_values[0])\n","\n","#     @tf.function\n","#     def replay(self, batch_size):\n","#         \"\"\"Parallelized experience replay for faster training\"\"\"\n","#         if len(self.memory) < batch_size:\n","#             return\n","#         minibatch = random.sample(self.memory, batch_size)\n","\n","#         states, targets = [], []\n","#         for state, action, reward, next_state, done in minibatch:\n","#             target = reward\n","#             if not done:\n","#                 target = reward + self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n","#             target_f = self.model.predict(state, verbose=0)\n","#             target_f[0][action] = target\n","#             states.append(state[0])\n","#             targets.append(target_f[0])\n","\n","#         # Convert to NumPy arrays for parallelized training\n","#         states = np.array(states)\n","#         targets = np.array(targets)\n","\n","#         # Train model in parallel\n","#         self.model.fit(states, targets, epochs=1, verbose=0, batch_size=batch_size)\n","\n","#         if self.epsilon > self.epsilon_min:\n","#             self.epsilon *= self.epsilon_decay\n","\n","# # Simulated Environment\n","# class JewelryEnvironment:\n","#     def __init__(self, jewelry_features):\n","#         self.jewelry_features = [f for f in jewelry_features.values() if f is not None and f.shape[0] == 1280]\n","#         self.action_size = len(self.jewelry_features)\n","#         self.state = None\n","\n","#     def reset(self, face_features):\n","#         \"\"\"Reset environment with a new face.\"\"\"\n","#         if face_features is None or face_features.shape[1] != 1280:\n","#             return np.zeros((1, 1280))\n","#         self.state = face_features.reshape(1, -1)\n","#         return self.state\n","\n","#     def step(self, action):\n","#         \"\"\"Take an action (select jewelry) and return reward, next state, done.\"\"\"\n","#         if action >= self.action_size:\n","#             action = 0\n","#         selected_jewelry = self.jewelry_features[action]\n","#         reward = self._simulate_reward(self.state, selected_jewelry)\n","#         next_state = self.state\n","#         done = True\n","#         return next_state, reward, done\n","\n","#     def _simulate_reward(self, face_features, jewelry_features):\n","#         \"\"\"Simulate reward using cosine similarity\"\"\"\n","#         face_features = face_features.reshape(-1)\n","#         jewelry_features = jewelry_features.reshape(-1)\n","#         if face_features.size != 1280 or jewelry_features.size != 1280:\n","#             return 0.0\n","#         cos_sim = np.dot(face_features, jewelry_features) / (np.linalg.norm(face_features) * np.linalg.norm(jewelry_features))\n","#         return float(cos_sim)\n","\n","# # Load Features and Train\n","# def train_rl_model():\n","#     # Load features\n","#     try:\n","#         multi_object_features = np.load(multi_object_features_path, allow_pickle=True).item()\n","#         pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","#     except Exception as e:\n","#         print(f\"❌ Error loading features: {e}\")\n","#         return\n","\n","#     face_features = [np.array(f) for f in multi_object_features.values() if f is not None and f.size == 1280]\n","#     jewelry_features = pairwise_features\n","\n","#     if not face_features or not jewelry_features:\n","#         print(\"❌ No valid features found, aborting training\")\n","#         return\n","\n","#     state_size = 1280\n","#     action_size = len(jewelry_features)\n","\n","#     agent = JewelryRLAgent(state_size, action_size)\n","#     env = JewelryEnvironment(jewelry_features)\n","\n","#     # Training loop with GPU optimization\n","#     episodes = 1000\n","#     batch_size = 32\n","#     dataset = tf.data.Dataset.from_generator(\n","#         lambda: ((random.choice(face_features),) for _ in range(episodes)),\n","#         output_signature=(tf.TensorSpec(shape=(1, 1280), dtype=tf.float32),)\n","#     ).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","#     for e, batch in enumerate(dataset):\n","#         state = batch.numpy()\n","#         action = agent.act(state)\n","#         next_state, reward, done = env.step(action)\n","\n","#         agent.remember(state, action, reward, next_state, done)\n","#         agent.replay(batch_size)\n","\n","#         if e % 50 == 0:\n","#             print(f\"Episode {e}/{episodes}, Epsilon: {agent.epsilon:.2f}, Reward: {reward:.2f}\")\n","\n","#     # Save model\n","#     agent.model.save(model_save_path)\n","#     print(f\"✅ Model saved to {model_save_path}\")\n","\n","# if __name__ == \"__main__\":\n","#     train_rl_model()\n"],"metadata":{"id":"hNMzSsHlAHJX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Predicting Suitability using the JewelryRLPredictor class**"],"metadata":{"id":"NW_gadw3xKC3"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model, Model\n","import pickle\n","\n","class JewelryRLPredictor:\n","    def __init__(self, model_path, scaler_path, pairwise_features_path):\n","        \"\"\"Initialize the RL predictor.\"\"\"\n","        self.model = load_model(model_path)\n","        self.img_size = (224, 224)\n","        self.device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","        self.feature_size = 1280  # Updated to match training\n","\n","        # Load scaler\n","        with open(scaler_path, 'rb') as f:\n","            self.scaler = pickle.load(f)\n","\n","        # Feature extractor\n","        base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","        global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n","        reduction_layer = tf.keras.layers.Dense(self.feature_size, activation=\"relu\")\n","        self.feature_extractor = Model(\n","            inputs=base_model.input,\n","            outputs=reduction_layer(global_avg_layer(base_model.output))\n","        )\n","\n","        # Load pairwise features\n","        self.pairwise_features = np.load(pairwise_features_path, allow_pickle=True).item()\n","        self.pairwise_features = {k: self.scaler.transform(np.array(v).reshape(1, -1)) for k, v in self.pairwise_features.items() if v is not None and v.size == 1280}\n","        self.jewelry_list = list(self.pairwise_features.values())\n","        self.jewelry_names = list(self.pairwise_features.keys())\n","\n","    def extract_features(self, img_path):\n","        \"\"\"Extract features from an image.\"\"\"\n","        if not os.path.exists(img_path):\n","            print(f\"❌ Error: Image file not found - {img_path}\")\n","            return None\n","        try:\n","            img = image.load_img(img_path, target_size=self.img_size)\n","            img_array = image.img_to_array(img)\n","            img_array = np.expand_dims(img_array, axis=0)\n","            img_array = preprocess_input(img_array)\n","            features = self.feature_extractor.predict(img_array, verbose=0)\n","            return self.scaler.transform(features)  # Shape: (1, 1280)\n","        except Exception as e:\n","            print(f\"⚠️ Error extracting features from {img_path}: {e}\")\n","            return None\n","\n","    def predict_compatibility(self, face_img_path, jewelry_img_path=None):\n","        \"\"\"Predict compatibility score.\"\"\"\n","        face_features = self.extract_features(face_img_path)\n","        if face_features is None:\n","            return None, None, None\n","\n","        # Use face features as state to predict Q-values\n","        with tf.device(self.device):\n","            q_values = self.model.predict(face_features, verbose=0)[0]\n","            action = np.argmax(q_values)  # Best jewelry action\n","            score = q_values[action]  # Predicted compatibility (Q-value)\n","\n","        print(f\"\\n🔮 **Predicted Compatibility Score**: {score:.2f}\")\n","\n","        # Recommend jewelry based on face features\n","        recommendations = self.recommend_jewelry(face_features)\n","        return score, action, recommendations\n","\n","    def recommend_jewelry(self, face_features, min_top_n=10, max_top_n=15):\n","        \"\"\"Recommend top 10-15 jewelry items.\"\"\"\n","        with tf.device(self.device):\n","            q_values = self.model.predict(face_features, verbose=0)[0]\n","\n","        # Get top actions (jewelry indices) sorted by Q-values\n","        top_indices = np.argsort(q_values)[::-1]\n","        total_options = len(self.jewelry_list)\n","        top_n = min(max(min_top_n, total_options), max_top_n)\n","        top_recommendations = [(self.jewelry_names[idx], q_values[idx]) for idx in top_indices[:top_n]]\n","\n","        # Sort alphabetically by name\n","        top_recommendations.sort(key=lambda x: x[0])\n","        print(f\"\\n🔮 **Top {top_n} Recommended Jewelry Image Names**\")\n","        for name, _ in top_recommendations:\n","            print(name)\n","\n","        return [name for name, _ in top_recommendations]\n","\n","# Usage\n","model_path = \"/content/drive/MyDrive/Jewelify/Trained Features/keras/rl_jewelry_model.keras\"\n","scaler_path = \"/content/drive/MyDrive/Jewelify/Trained Features/rl_scaler.pkl\"\n","pairwise_features_path = \"/content/drive/MyDrive/Jewelify/Trained Features/pandas/pairwise_features.npy\"\n","\n","predictor = JewelryRLPredictor(model_path, scaler_path, pairwise_features_path)\n","\n","# Test prediction and recommendation\n","score, action, recommendations = predictor.predict_compatibility(\n","    face_img_path=\"/content/girl_6.jpg\",\n","    jewelry_img_path=\"/content/download (4).jpg\"  # Ignored in RL setup\n","    # jewelry_img_path=\"/content/download.jpg\"  # Ignored in RL setup\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOeRihJKw8MI","executionInfo":{"status":"ok","timestamp":1740570647261,"user_tz":-330,"elapsed":1122,"user":{"displayName":"smit vaghasiya nagajibhai","userId":"15314800821480743640"}},"outputId":"e6a34acd-211e-464b-dba4-dd52e505642d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["❌ Error: Image file not found - /content/girl_6.jpg\n"]}]},{"cell_type":"code","source":["\n","# predictor = JewelryRLPredictor(model_path, scaler_path, pairwise_features_path)\n","\n","# # Test prediction and recommendation\n","# score, action, recommendations = predictor.predict_compatibility(\n","#     face_img_path=\"/content/girl_7.jpg\",\n","#     jewelry_img_path=\"/content/download (7).jpg\"  # Ignored in RL setup\n","# )"],"metadata":{"id":"g8ZREznxxBn9"},"execution_count":null,"outputs":[]}]}